{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS0c34ZzssXSWXPhGKwkmL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d63cf7d470541038a6e12720e59e4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68c7e1387416462590a954e9f3d7947b",
              "IPY_MODEL_7b76cf90725d4f31835ceb27d80a599d",
              "IPY_MODEL_2ef17dcccc5f4c41816d52eb87f54a41"
            ],
            "layout": "IPY_MODEL_7f47a86a8b2e422d8458670f591b32bd"
          }
        },
        "68c7e1387416462590a954e9f3d7947b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_546e189a2f0743c6a96ce9a7e33e2a2a",
            "placeholder": "​",
            "style": "IPY_MODEL_75215d27972b4ca2b570c2f21190f010",
            "value": "Generating train split: "
          }
        },
        "7b76cf90725d4f31835ceb27d80a599d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e26c697129f941038a733aa0edc406ba",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2492b416023c417e95d4fe874bbfa4f7",
            "value": 1
          }
        },
        "2ef17dcccc5f4c41816d52eb87f54a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_605971980f9845b1bb4ddf60e956bc7e",
            "placeholder": "​",
            "style": "IPY_MODEL_634ae4065c6c426b98ad8eb429b20d46",
            "value": " 90/0 [00:00&lt;00:00, 2408.75 examples/s]"
          }
        },
        "7f47a86a8b2e422d8458670f591b32bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "546e189a2f0743c6a96ce9a7e33e2a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75215d27972b4ca2b570c2f21190f010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e26c697129f941038a733aa0edc406ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2492b416023c417e95d4fe874bbfa4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "605971980f9845b1bb4ddf60e956bc7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "634ae4065c6c426b98ad8eb429b20d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PratyushAggarwal1/BiteSpeed/blob/main/NaptickTask2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8d63cf7d470541038a6e12720e59e4a2",
            "68c7e1387416462590a954e9f3d7947b",
            "7b76cf90725d4f31835ceb27d80a599d",
            "2ef17dcccc5f4c41816d52eb87f54a41",
            "7f47a86a8b2e422d8458670f591b32bd",
            "546e189a2f0743c6a96ce9a7e33e2a2a",
            "75215d27972b4ca2b570c2f21190f010",
            "e26c697129f941038a733aa0edc406ba",
            "2492b416023c417e95d4fe874bbfa4f7",
            "605971980f9845b1bb4ddf60e956bc7e",
            "634ae4065c6c426b98ad8eb429b20d46"
          ]
        },
        "id": "lK-IceUo0jMA",
        "outputId": "c42cca68-85da-47ee-f5e2-56e866eb9d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Installing STT, TTS, and Audio Handling libraries...\n",
            "⏳ Installing Core ML, LLM, and Fine-Tuning libraries...\n",
            "   Attempting to upgrade 'datasets' library...\n",
            "⏳ Installing TTS library...\n",
            "\n",
            "--- Main Installations Attempted ---\n",
            "\n",
            "⏳ Forcing fsspec version compatible with 'datasets'...\n",
            "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "⏳ Installing Gradio\n",
            "\n",
            "✅ All dependency installations and conflict resolutions attempted.\n",
            "   ⚠️ NOTE: Expect a potential conflict warning regarding 'gcsfs' needing a different fsspec version.\n",
            "   We will proceed and see if this causes issues during model/data loading or training.\n",
            "\n",
            "--- Verifying GPU Access ---\n",
            "❌ WARNING: No GPU detected by PyTorch!\n",
            "   >>> Action Required: Go to 'Runtime' -> 'Change runtime type', select 'T4 GPU', and click 'Save'. <<<\n",
            "   Then, RESTART the runtime ('Runtime' -> 'Restart runtime') and run this cell again.\n",
            "   Proceeding without a GPU will make LLM operations extremely slow.\n",
            "--- GPU Verification Complete ---\n",
            "⏳ Loading Whisper STT model 'base.en' onto cpu (compute: int8)...\n",
            "✅ Whisper STT model ('base.en') loaded successfully.\n",
            "Done\n",
            "--- TTS Configuration ---\n",
            "Repo ID: rhasspy/piper-voices\n",
            "Voice Path Prefix: en/en_US/lessac/medium\n",
            "Onnx File: en_US-lessac-medium.onnx\n",
            "Json File: en_US-lessac-medium.onnx.json\n",
            "-------------------------\n",
            "\n",
            "⏳ Downloading TTS files using huggingface_hub (if needed)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Copied ONNX file to expected path: piper_models/en_US-lessac-medium.onnx\n",
            "   Copied JSON file to expected path: piper_models/en_US-lessac-medium.onnx.json\n",
            "\n",
            " Verifying local files again before loading...\n",
            "\n",
            "⏳ Loading TTS voice model into memory...\n",
            "✅ TTS voice model 'en_US-lessac-medium' loaded successfully.\n",
            "   Retrieved Sample Rate: 22050 Hz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged into Hugging Face Hub\n",
            "⏳ Loading base LLM: 'mistralai/Mistral-7B-Instruct-v0.2' with 4-bit quantization...\n",
            "   This requires downloading several GB and can take 5-15 minutes...\n",
            "❌ Error loading base LLM model: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
            "   >>> COMMON FIXES: <<<\n",
            "   1. OUT OF MEMORY: Go to 'Runtime' -> 'Disconnect and delete runtime', then reconnect and run cells again.\n",
            "   2. Ensure T4 GPU is selected in Runtime settings.\n",
            "   3. Close other Colab notebooks or browser tabs consuming memory.\n",
            "   4. If errors persist, Colab might be temporarily overloaded; try again later.\n",
            "\n",
            "Skipping tokenizer loading because the base LLM failed to load.\n",
            "\n",
            "✅ 'Listen' functions defined (transcribe_audio, transcribe_uploaded_audio).\n",
            "\n",
            "✅ 'Think' function defined (get_llm_response) with history management.\n",
            "\n",
            "✅ 'Speak' function defined (speak_text_and_display).\n",
            "✅ Gradio-specific TTS function 'synthesize_for_gradio_and_autoplay' defined.\n",
            "\n",
            "💾 Saving 90 examples to 'sleep_coach_finetuning_data.jsonl'...\n",
            "✅ Data saved successfully.\n",
            "\n",
            "⏳ Loading dataset from 'sleep_coach_finetuning_data.jsonl'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d63cf7d470541038a6e12720e59e4a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset loaded successfully:\n",
            "   Number of examples: 90\n",
            "\n",
            "   First example in loaded dataset:\n",
            "{'instruction': 'User data: Slept 5 hours 10 minutes, 45 mins deep sleep, 1 hour REM. Felt very tired waking up. Question: What should I focus on?', 'response': \"Getting only around 5 hours isn't ideal for most adults, which likely explains the tiredness. Focus on extending your total sleep time towards the 7-9 hour range. A consistent wind-down routine before bed might help you fall asleep faster and get more restorative deep and REM sleep.\"}\n",
            "--- Checking Prerequisites for Fine-Tuning ---\n",
            "❌ Prerequisite Failed: Base LLM model ('llm_model') not loaded. Run Step 4.\n",
            "❌ Prerequisite Failed: LLM Tokenizer ('llm_tokenizer') not loaded. Run Step 4.\n",
            "\n",
            "🛑 Cannot proceed with fine-tuning due to missing prerequisites.\n",
            "Base model not found in memory. Attempting reload...\n",
            "⏳ Reloading base model 'mistralai/Mistral-7B-Instruct-v0.2' with quantization...\n",
            "❌ Failed to reload base model/tokenizer: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
            "❌ Error: Fine-tuned adapter path not found at './sleep-coach-lora-adapter-mistral7b/final_adapter'. Did Step 9 complete and save correctly?\n",
            "--- Checking Prerequisites for Interaction ---\n",
            "❌ Prerequisite Failed: Fine-tuned model ('model_finetuned') not loaded. Run Step 10.\n",
            "❌ Prerequisite Failed: LLM Tokenizer ('llm_tokenizer') not loaded. Ensure Step 4 or 10 succeeded.\n",
            "\n",
            "🛑 Cannot start interaction loop due to missing prerequisites.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-6abe11db09cc>:1356: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_ui = gr.Chatbot(\n",
            "<ipython-input-25-6abe11db09cc>:1356: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot_ui = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Launching Gradio Interface in Colab...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://07dd1f7999b8ba07e0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://07dd1f7999b8ba07e0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://07dd1f7999b8ba07e0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Naptick_Task2.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1sub3Kuwk4MbfOuGeZN_IuzROLd1G9zXI\n",
        "\"\"\"\n",
        "\n",
        "print(\"⏳ Installing STT, TTS, and Audio Handling libraries...\")\n",
        "!pip install -q faster-whisper soundfile SpeechRecognition ipython\n",
        "\n",
        "print(\"⏳ Installing Core ML, LLM, and Fine-Tuning libraries...\")\n",
        "# Keep datasets upgrade attempt just in case, though it didn't fix the core issue before\n",
        "print(\"   Attempting to upgrade 'datasets' library...\")\n",
        "!pip install -q -U datasets\n",
        "\n",
        "# Now install the rest of the ML stack\n",
        "!pip install -q transformers accelerate peft bitsandbytes sentencepiece torch\n",
        "\n",
        "print(\"⏳ Installing TTS library...\")\n",
        "!pip install -q piper-tts\n",
        "\n",
        "print(\"\\n--- Main Installations Attempted ---\")\n",
        "\n",
        "# --- Dependency Conflict Resolution ---\n",
        "# Reasoning: Force install the version of fsspec REQUIRED BY 'datasets'.\n",
        "# This WILL likely conflict with 'gcsfs', but we prioritize the libraries\n",
        "# WE are directly using (datasets, transformers, etc.) and see if the gcsfs conflict\n",
        "# actually causes a runtime error later in our specific workflow.\n",
        "print(\"\\n⏳ Forcing fsspec version compatible with 'datasets'...\")\n",
        "!pip install -U fsspec==2025.3.0 # <-- FORCE version needed by datasets\n",
        "print(\"⏳ Installing Gradio\")\n",
        "!pip install -q gradio\n",
        "print(\"\\n✅ All dependency installations and conflict resolutions attempted.\")\n",
        "print(\"   ⚠️ NOTE: Expect a potential conflict warning regarding 'gcsfs' needing a different fsspec version.\")\n",
        "print(\"   We will proceed and see if this causes issues during model/data loading or training.\")\n",
        "\n",
        "import torch\n",
        "print(\"\\n--- Verifying GPU Access ---\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"✅ GPU detected: {gpu_name}\")\n",
        "    # Display PyTorch and CUDA versions for debugging purposes.\n",
        "    print(f\"   PyTorch Version: {torch.__version__}\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "    print(\"   GPU is ready for model loading and computations.\")\n",
        "else:\n",
        "    print(\"❌ WARNING: No GPU detected by PyTorch!\")\n",
        "    print(\"   >>> Action Required: Go to 'Runtime' -> 'Change runtime type', select 'T4 GPU', and click 'Save'. <<<\")\n",
        "    print(\"   Then, RESTART the runtime ('Runtime' -> 'Restart runtime') and run this cell again.\")\n",
        "    print(\"   Proceeding without a GPU will make LLM operations extremely slow.\")\n",
        "print(\"--- GPU Verification Complete ---\")\n",
        "\n",
        "import faster_whisper\n",
        "import torch\n",
        "import os\n",
        "stt_model_size = \"base.en\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if torch.cuda.is_available() else \"int8\"\n",
        "stt_model = None # Initialize variable to hold the model\n",
        "print(f\"⏳ Loading Whisper STT model '{stt_model_size}' onto {device} (compute: {compute_type})...\")\n",
        "try:\n",
        "    stt_model = faster_whisper.WhisperModel(stt_model_size, device=device, compute_type=compute_type)\n",
        "    print(f\"✅ Whisper STT model ('{stt_model_size}') loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading Whisper model: {e}\")\n",
        "    print(\"   This could be due to insufficient GPU memory. Consider trying a smaller model size (e.g., 'tiny.en').\")\n",
        "    print(\"   Also ensure the GPU runtime is active.\")\n",
        "\n",
        "!pip install -q -U huggingface_hub\n",
        "print(\"Done\")\n",
        "\n",
        "# @title Step 3: Load TTS Model (Piper) - FINAL WORKING VERSION\n",
        "\n",
        "# --- Ensure latest huggingface_hub is installed (optional here, but good practice) ---\n",
        "# print(\"⏳ Upgrading huggingface_hub library...\")\n",
        "# !pip install -q -U huggingface_hub\n",
        "# print(\"✅ huggingface_hub upgrade attempt complete.\")\n",
        "\n",
        "from pathlib import Path\n",
        "from piper import PiperVoice\n",
        "import os\n",
        "import time\n",
        "try:\n",
        "    from huggingface_hub import hf_hub_download\n",
        "except ImportError:\n",
        "    print(\"❌ ERROR: huggingface_hub library not found. Please install it: !pip install -q huggingface_hub\")\n",
        "    raise SystemExit(\"Stopping: huggingface_hub not installed.\")\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "voice_repo_id = \"rhasspy/piper-voices\"\n",
        "voice_lang = \"en\"\n",
        "voice_region = \"en_US\"\n",
        "voice_family = \"lessac\"\n",
        "voice_quality = \"medium\"\n",
        "voice_repo_path_prefix = f\"{voice_lang}/{voice_region}/{voice_family}/{voice_quality}\"\n",
        "voice_name = f\"{voice_region}-{voice_family}-{voice_quality}\"\n",
        "onnx_filename = f\"{voice_name}.onnx\"\n",
        "json_filename = f\"{voice_name}.onnx.json\"\n",
        "\n",
        "print(f\"--- TTS Configuration ---\")\n",
        "print(f\"Repo ID: {voice_repo_id}\")\n",
        "print(f\"Voice Path Prefix: {voice_repo_path_prefix}\")\n",
        "print(f\"Onnx File: {onnx_filename}\")\n",
        "print(f\"Json File: {json_filename}\")\n",
        "print(f\"-------------------------\")\n",
        "\n",
        "force_redownload = False # Keep False unless needed\n",
        "\n",
        "model_dir = Path(\"./piper_models\")\n",
        "model_dir.mkdir(exist_ok=True)\n",
        "voice_model_path = model_dir / onnx_filename\n",
        "voice_config_path = model_dir / json_filename\n",
        "\n",
        "if force_redownload:\n",
        "    print(\"🧹 Force redownload enabled. Deleting existing local TTS model files...\")\n",
        "    # (Deletion code kept for future use if needed)\n",
        "    if voice_model_path.is_file():\n",
        "        try: os.remove(voice_model_path); print(f\"   Deleted: {voice_model_path}\")\n",
        "        except OSError as e: print(f\"   Warning: Could not delete {voice_model_path}: {e}\")\n",
        "    if voice_config_path.is_file():\n",
        "        try: os.remove(voice_config_path); print(f\"   Deleted: {voice_config_path}\")\n",
        "        except OSError as e: print(f\"   Warning: Could not delete {voice_config_path}: {e}\")\n",
        "    print(\"   Deletion attempt complete.\")\n",
        "\n",
        "# --- Download using hf_hub_download ---\n",
        "print(f\"\\n⏳ Downloading TTS files using huggingface_hub (if needed)...\")\n",
        "download_ok = True\n",
        "actual_onnx_path_returned = None\n",
        "actual_json_path_returned = None\n",
        "try:\n",
        "    onnx_repo_filepath = f\"{voice_repo_path_prefix}/{onnx_filename}\"\n",
        "    json_repo_filepath = f\"{voice_repo_path_prefix}/{json_filename}\"\n",
        "\n",
        "    # Download ONNX\n",
        "    actual_onnx_path_returned = hf_hub_download(\n",
        "        repo_id=voice_repo_id, filename=onnx_repo_filepath, cache_dir=None,\n",
        "        force_download=force_redownload, local_dir=str(model_dir), local_dir_use_symlinks=False\n",
        "    )\n",
        "    if Path(actual_onnx_path_returned).resolve() != voice_model_path.resolve():\n",
        "         import shutil\n",
        "         try:\n",
        "            shutil.copyfile(actual_onnx_path_returned, voice_model_path)\n",
        "            print(f\"   Copied ONNX file to expected path: {voice_model_path}\")\n",
        "         except Exception as copy_e:\n",
        "             print(f\"   Warning: Failed to copy ONNX file: {copy_e}. Using cached path: {actual_onnx_path_returned}\")\n",
        "             voice_model_path = Path(actual_onnx_path_returned)\n",
        "    else:\n",
        "        print(f\"   ONNX file found/verified at: {voice_model_path}\")\n",
        "\n",
        "\n",
        "    # Download JSON\n",
        "    actual_json_path_returned = hf_hub_download(\n",
        "        repo_id=voice_repo_id, filename=json_repo_filepath, cache_dir=None,\n",
        "        force_download=force_redownload, local_dir=str(model_dir), local_dir_use_symlinks=False\n",
        "    )\n",
        "    if Path(actual_json_path_returned).resolve() != voice_config_path.resolve():\n",
        "         import shutil\n",
        "         try:\n",
        "            shutil.copyfile(actual_json_path_returned, voice_config_path)\n",
        "            print(f\"   Copied JSON file to expected path: {voice_config_path}\")\n",
        "         except Exception as copy_e:\n",
        "             print(f\"   Warning: Failed to copy JSON file: {copy_e}. Using cached path: {actual_json_path_returned}\")\n",
        "             voice_config_path = Path(actual_json_path_returned)\n",
        "    else:\n",
        "         print(f\"   JSON file found/verified at: {voice_config_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during hf_hub_download process: {e}\")\n",
        "    download_ok = False\n",
        "\n",
        "# --- Verification AFTER Download Attempt ---\n",
        "print(\"\\n Verifying local files again before loading...\")\n",
        "model_ok = False\n",
        "config_ok = False\n",
        "min_file_size = 100\n",
        "\n",
        "if voice_model_path.is_file() and os.path.getsize(voice_model_path) > min_file_size: model_ok = True\n",
        "if voice_config_path.is_file() and os.path.getsize(voice_config_path) > 0: config_ok = True\n",
        "\n",
        "if not model_ok: print(f\"   ❌ Verification Failed: Model file issue at {voice_model_path}\")\n",
        "if not config_ok: print(f\"   ❌ Verification Failed: Config file issue at {voice_config_path}\")\n",
        "\n",
        "# --- Load Piper Voice (Using Correct Attribute) ---\n",
        "tts_voice = None\n",
        "tts_sample_rate = 22050 # Keep default as fallback\n",
        "\n",
        "if model_ok and config_ok:\n",
        "    print(\"\\n⏳ Loading TTS voice model into memory...\")\n",
        "    try:\n",
        "        tts_voice = PiperVoice.load(str(voice_model_path), str(voice_config_path))\n",
        "        # --- Use the CORRECT attribute to get sample rate ---\n",
        "        if hasattr(tts_voice, 'config') and hasattr(tts_voice.config, 'sample_rate'):\n",
        "            tts_sample_rate = tts_voice.config.sample_rate\n",
        "            print(f\"✅ TTS voice model '{voice_name}' loaded successfully.\")\n",
        "            print(f\"   Retrieved Sample Rate: {tts_sample_rate} Hz\")\n",
        "        else:\n",
        "            # This case should not happen based on previous debug, but included for safety\n",
        "            print(f\"✅ TTS voice model '{voice_name}' loaded, but couldn't retrieve sample rate automatically.\")\n",
        "            print(f\"   Using default Sample Rate: {tts_sample_rate} Hz\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading TTS model: {e}\")\n",
        "else:\n",
        "    print(\"\\n❌ Skipping TTS model loading due to file verification failure.\")\n",
        "\n",
        "import huggingface_hub\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "try:\n",
        "  hf_token=userdata.get('HF_TOKEN')\n",
        "  huggingface_hub.login(token=hf_token,add_to_git_credential=True)\n",
        "  print(\"Successfully logged into Hugging Face Hub\")\n",
        "except userdata.SecretNotFoundError:\n",
        "  print(\"HF_Token secret not found\")\n",
        "except Exception as e:\n",
        "  print(\"An error occured during login\")\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# --- Configuration ---\n",
        "# Reasoning: Specify the model identifier from the Hugging Face Hub.\n",
        "# \"mistralai/Mistral-7B-Instruct-v0.2\" is chosen for its strong performance in conversation/instruction-following\n",
        "# and its relatively manageable size (7 billion parameters) when quantized.\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# --- Quantization Configuration (bitsandbytes) ---\n",
        "# Reasoning: Define how to load the model with reduced precision to save memory.\n",
        "# load_in_4bit=True: Activates 4-bit quantization.\n",
        "# bnb_4bit_quant_type=\"nf4\": Specifies the \"NormalFloat 4-bit\" quantization type, a good default.\n",
        "# bnb_4bit_compute_dtype=torch.bfloat16: Sets the data type used for computations *during* inference/training.\n",
        "#    bfloat16 offers a good balance of speed and stability on compatible GPUs (like Colab's T4).\n",
        "# bnb_4bit_use_double_quant=True: A technique that can save a small amount of additional memory.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# --- Load Base LLM Model ---\n",
        "llm_model = None # Initialize variable\n",
        "print(f\"⏳ Loading base LLM: '{base_model_id}' with 4-bit quantization...\")\n",
        "print(\"   This requires downloading several GB and can take 5-15 minutes...\")\n",
        "try:\n",
        "    # Reasoning: Load the Causal Language Model using AutoModelForCausalLM.\n",
        "    # quantization_config=bnb_config: Applies the 4-bit loading defined above.\n",
        "    # device_map=\"auto\": Tells Accelerate to automatically distribute the model layers across available devices (primarily the GPU).\n",
        "    # trust_remote_code=True: Sometimes required for models with custom code defining their architecture.\n",
        "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    # Verify model loading and device placement\n",
        "    print(f\"✅ Base LLM model loaded successfully.\")\n",
        "    print(f\"   Model is on device(s): {llm_model.hf_device_map}\") # Show device map\n",
        "\n",
        "except Exception as e:\n",
        "    # Reasoning: Catch errors, especially Out-of-Memory (OOM), which are common on resource-constrained environments like Colab free tier.\n",
        "    print(f\"❌ Error loading base LLM model: {e}\")\n",
        "    print(\"   >>> COMMON FIXES: <<<\")\n",
        "    print(\"   1. OUT OF MEMORY: Go to 'Runtime' -> 'Disconnect and delete runtime', then reconnect and run cells again.\")\n",
        "    print(\"   2. Ensure T4 GPU is selected in Runtime settings.\")\n",
        "    print(\"   3. Close other Colab notebooks or browser tabs consuming memory.\")\n",
        "    print(\"   4. If errors persist, Colab might be temporarily overloaded; try again later.\")\n",
        "    llm_model = None # Ensure variable is None if loading failed\n",
        "\n",
        "# --- Load LLM Tokenizer ---\n",
        "llm_tokenizer = None # Initialize variable\n",
        "# Reasoning: Only proceed if the model loaded successfully.\n",
        "if llm_model:\n",
        "    print(\"\\n⏳ Loading LLM Tokenizer...\")\n",
        "    try:\n",
        "        # Reasoning: Load the tokenizer associated with the specific base model.\n",
        "        # The tokenizer converts text into numerical sequences (tokens) the model understands.\n",
        "        llm_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "        # --- Tokenizer Configuration ---\n",
        "        # Reasoning: Set the padding token ID. For many causal LMs, the End-Of-Sentence (EOS) token\n",
        "        # is used for padding. This tells the model to ignore these tokens during processing.\n",
        "        llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
        "        # Reasoning: Set the side on which padding is added. 'right' is often preferred for causal LMs\n",
        "        # during generation to prevent issues.\n",
        "        llm_tokenizer.padding_side = \"right\"\n",
        "\n",
        "        print(\"✅ LLM Tokenizer loaded and configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading tokenizer: {e}\")\n",
        "        llm_tokenizer = None\n",
        "else:\n",
        "    print(\"\\nSkipping tokenizer loading because the base LLM failed to load.\")\n",
        "\n",
        "# @title Step 5: Define \"Listen\" Function (Audio Transcription)\n",
        "# Reasoning: Define the functions that handle audio input and convert it to text using the loaded Whisper model.\n",
        "\n",
        "from google.colab import files # For Colab's file upload UI.\n",
        "import io # For handling byte streams (though not directly used here, often useful).\n",
        "import soundfile as sf # For potentially reading audio file info if needed.\n",
        "import numpy as np # Whisper often works well with NumPy arrays.\n",
        "import time # For timing the transcription process.\n",
        "import os # For file operations (checking existence, removing temp files).\n",
        "\n",
        "# --- Core Transcription Function ---\n",
        "def transcribe_audio(audio_path_or_numpy_array):\n",
        "    \"\"\"\n",
        "    Transcribes audio using the loaded Whisper STT model.\n",
        "\n",
        "    Args:\n",
        "        audio_path_or_numpy_array:`` Path to the audio file OR a NumPy array containing audio data.\n",
        "\n",
        "    Returns:\n",
        "        The transcribed text (str) or None if transcription fails.\n",
        "    \"\"\"\n",
        "    # Reasoning: Safety check to ensure the STT model object is available.\n",
        "    if not stt_model:\n",
        "        print(\"❌ Error: Whisper STT model ('stt_model') is not loaded.\")\n",
        "        return None\n",
        "    try:\n",
        "        print(f\"\\n⏳ Transcribing audio...\")\n",
        "        start_time = time.time()\n",
        "        # Reasoning: Call the transcribe method of the loaded faster-whisper model.\n",
        "        # beam_size=5: A common setting for beam search decoding, balances speed and accuracy.\n",
        "        # language=\"en\": Hints that the expected language is English (can improve accuracy if known).\n",
        "        segments, info = stt_model.transcribe(audio_path_or_numpy_array, beam_size=5, language=\"en\")\n",
        "\n",
        "        # Reasoning: The model returns segments; concatenate their text attributes to form the full transcription.\n",
        "        # .strip() removes leading/trailing whitespace.\n",
        "        transcription = \"\".join(segment.text for segment in segments).strip()\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        # Reasoning: Provide informative output about the transcription process.\n",
        "        print(f\"   🗣️ Transcription: '{transcription}'\")\n",
        "        print(f\"   🌐 Detected language: {info.language} (Confidence: {info.language_probability:.2f})\")\n",
        "        # print(f\"   ⏱️ (Transcription time: {duration:.2f} seconds)\") # Optional: uncomment for timing info\n",
        "        return transcription\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during Whisper transcription: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Colab Upload Handler ---\n",
        "def transcribe_uploaded_audio():\n",
        "    \"\"\"\n",
        "    Handles audio file upload in Colab, transcribes it, and cleans up.\n",
        "\n",
        "    Returns:\n",
        "        The transcribed text (str) or None if the process fails.\n",
        "    \"\"\"\n",
        "    print(\"\\n🎙️ Please upload an audio file (e.g., WAV, MP3, M4A)...\")\n",
        "    # Reasoning: Use Colab's built-in 'files.upload()' to trigger the browser's upload dialog.\n",
        "    # This returns a dictionary where keys are filenames and values are file content in bytes.\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"   No file uploaded or upload cancelled.\")\n",
        "        return None\n",
        "\n",
        "    # Assume only one file is uploaded, get its name and byte content.\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    audio_bytes = uploaded[file_name]\n",
        "    print(f\"   File '{file_name}' uploaded ({len(audio_bytes)} bytes).\")\n",
        "\n",
        "    # Reasoning: Define a temporary path within Colab's filesystem to save the uploaded bytes.\n",
        "    # While faster-whisper can sometimes handle bytes directly, using a file path is often more robust.\n",
        "    temp_audio_path = f\"./temp_audio_for_transcription_{file_name}\"\n",
        "    transcription_result = None\n",
        "    try:\n",
        "        # Write the uploaded bytes to the temporary file.\n",
        "        with open(temp_audio_path, 'wb') as f:\n",
        "            f.write(audio_bytes)\n",
        "        # Call the core transcription function using the path to the temporary file.\n",
        "        transcription_result = transcribe_audio(temp_audio_path)\n",
        "    except Exception as e:\n",
        "         print(f\"   ❌ Error processing or writing uploaded file: {e}\")\n",
        "         transcription_result = None\n",
        "    finally:\n",
        "        # Reasoning: CRITICAL - Always attempt to delete the temporary file afterwards to avoid cluttering\n",
        "        # Colab's limited disk space, regardless of whether transcription succeeded or failed.\n",
        "        if os.path.exists(temp_audio_path):\n",
        "            try:\n",
        "                os.remove(temp_audio_path)\n",
        "                # print(f\"   🧹 Temporary file '{temp_audio_path}' deleted.\") # Optional: uncomment for confirmation\n",
        "            except OSError as e:\n",
        "                # Non-critical error if deletion fails, just log a warning.\n",
        "                print(f\"   ⚠️ Warning: Could not delete temporary file {temp_audio_path}: {e}\")\n",
        "\n",
        "    return transcription_result\n",
        "\n",
        "print(\"\\n✅ 'Listen' functions defined (transcribe_audio, transcribe_uploaded_audio).\")\n",
        "# You can uncomment the example usage lines in the previous version's Step 5 cell to test upload now if desired.\n",
        "\n",
        "# Example Usage (Optional - uncomment to test)\n",
        "# print(\"\\n--- Testing Audio Upload ---\")\n",
        "# text_result = transcribe_uploaded_audio()\n",
        "# if text_result:\n",
        "#     print(f\"\\n✅ Successfully transcribed: '{text_result}'\")\n",
        "# else:\n",
        "#     print(\"\\n❌ Audio transcription test failed.\")\n",
        "# print(\"--- End Test ---\")\n",
        "\n",
        "# @title Step 6: Define \"Think\" Function (LLM Interaction & History Management)\n",
        "# Reasoning: Define the function that takes user input, interacts with the LLM (base or fine-tuned),\n",
        "# manages conversation history for context, and returns the LLM's response.\n",
        "\n",
        "import time\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList # Potentially useful later, import now\n",
        "\n",
        "# --- Conversation History Storage ---\n",
        "# Reasoning: Use simple Python lists to store the conversation turns. Each turn is a dictionary\n",
        "# with 'role' ('user' or 'assistant') and 'content' (the text). Maintain separate lists\n",
        "# to allow comparison between the base model and the fine-tuned model later.\n",
        "conversation_history_base = []\n",
        "conversation_history_finetuned = []\n",
        "\n",
        "# --- System Prompt Definition ---\n",
        "# Reasoning: Define the guiding instructions for the LLM's behavior and persona.\n",
        "# This is prepended to the conversation history (conceptually or explicitly).\n",
        "# It sets constraints (no medical advice), defines the role (sleep coach), and suggests tone.\n",
        "SYSTEM_PROMPT = \"\"\"You are 'NapCoach', a specialized Sleep Coaching Assistant running on a Mistral-based model. Your primary function is to provide helpful, safe, accurate, and encouraging information related to sleep health, habits, and interpreting general sleep patterns based on user queries and provided data.\n",
        "\n",
        "**Your Core Directives:**\n",
        "1.  **Focus:** Stick strictly to sleep-related topics (hygiene, habits, cycles, interpreting non-medical data trends).\n",
        "2.  **Safety First:** NEVER provide medical advice, diagnoses, or treatment recommendations. If a user asks for medical advice or describes potentially serious symptoms (e.g., severe insomnia, sleep apnea symptoms like gasping), YOU MUST respond with: \"For medical concerns or diagnoses, it's essential to consult with a qualified healthcare professional.\" Do not elaborate further on the medical topic.\n",
        "3.  **Data Interpretation:** If provided with sleep data (wearable summaries, diary entries), interpret general trends and compare them to typical healthy ranges (e.g., \"Your sleep efficiency of 88% is generally considered good.\"). Avoid definitive statements about health based solely on data.\n",
        "4.  **Knowledge Base:** Rely ONLY on the information present in the conversation history and your training data. Do not invent facts or statistics. If you don't know something specific, state: \"I don't have specific information on that topic, but I can discuss general sleep principles.\"\n",
        "5.  **Persona:** Be friendly, supportive, knowledgeable (within bounds), and concise. Encourage good sleep habits.\n",
        "6.  **Formatting:** Keep responses well-structured and relatively brief.\n",
        "\"\"\"\n",
        "\n",
        "# --- Mistral Prompt Formatting Function ---\n",
        "# Reasoning: Mistral Instruct models require a specific format using special tags ([INST], [/INST], <s>, </s>).\n",
        "# This function takes a list of history messages and constructs the correctly formatted string.\n",
        "def format_prompt_mistral(messages):\n",
        "    \"\"\"Formats a list of message dictionaries into the Mistral Instruct prompt format.\"\"\"\n",
        "    prompt_str = \"\"\n",
        "    if not messages: return \"\"\n",
        "\n",
        "    # Handle potential system message if it's the first item\n",
        "    if messages[0][\"role\"] == \"system\":\n",
        "        # Mistral doesn't have an explicit system role tag in the same way as some others.\n",
        "        # The convention is often to prepend the system instructions to the *first* user message.\n",
        "        # We'll handle this conceptually in get_llm_response by ensuring the system prompt is considered.\n",
        "        messages = messages[1:] # Remove system message from turn list for formatting loop\n",
        "\n",
        "    # Construct the string turn-by-turn\n",
        "    for i, msg in enumerate(messages):\n",
        "        role = msg[\"role\"]\n",
        "        content = msg[\"content\"]\n",
        "        if role == \"user\":\n",
        "            # Add BOS token <s> only for the very first instruction turn in the formatted string\n",
        "            prefix = \"<s>\" if i == 0 else \"\"\n",
        "            prompt_str += f\"{prefix}[INST] {content} [/INST]\"\n",
        "        elif role == \"assistant\":\n",
        "            # Add EOS token </s> after every assistant response\n",
        "            prompt_str += f\" {content}</s>\"\n",
        "            # If the *next* message is also from the assistant (shouldn't happen with proper history),\n",
        "            # we might need another separator, but standard user/assistant alternation avoids this.\n",
        "\n",
        "    # The final prompt string should end with [/INST] (the user's last message),\n",
        "    # making it ready for the model to generate the assistant's response.\n",
        "    return prompt_str\n",
        "\n",
        "# --- Core LLM Response Generation Function ---\n",
        "def get_llm_response(user_message, model_to_use, tokenizer_to_use, history, is_finetuned_model=False):\n",
        "    \"\"\"\n",
        "    Gets a response from the specified LLM, manages history, and handles formatting.\n",
        "\n",
        "    Args:\n",
        "        user_message (str): The user's latest input text.\n",
        "        model_to_use: The loaded LLM model object (base or fine-tuned).\n",
        "        tokenizer_to_use: The loaded tokenizer object corresponding to the model.\n",
        "        history (list): The list storing the conversation history for this model.\n",
        "        is_finetuned_model (bool): Flag to indicate if the fine-tuned model is being used (for logging).\n",
        "\n",
        "    Returns:\n",
        "        The generated assistant response (str) or an error message.\n",
        "    \"\"\"\n",
        "    # Reasoning: Perform checks to ensure model and tokenizer objects are valid before proceeding.\n",
        "    if not model_to_use or not tokenizer_to_use:\n",
        "        print(\"❌ Error: LLM model or tokenizer is not available.\")\n",
        "        return \"My apologies, I'm unable to process your request right now as my core module isn't ready.\"\n",
        "\n",
        "    model_identifier = \"Fine-tuned Model\" if is_finetuned_model else \"Base Model\"\n",
        "    print(f\"\\n🤔 Thinking... (Using {model_identifier})\")\n",
        "\n",
        "    # Reasoning: Manage history. Prepend system prompt if history is empty. Add current user message.\n",
        "    current_turn_history = list(history) # Make a copy to potentially modify for this turn\n",
        "    if not current_turn_history:\n",
        "        # If history is empty, conceptually add the system prompt.\n",
        "        # We will prepend it to the user message for the Mistral format.\n",
        "         current_turn_history.append({\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\nUser query: {user_message}\"})\n",
        "         history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT}) # Add system to actual history\n",
        "         history.append({\"role\": \"user\", \"content\": user_message}) # Add user msg to actual history\n",
        "    else:\n",
        "         # History already exists, just add the user message\n",
        "         current_turn_history.append({\"role\": \"user\", \"content\": user_message})\n",
        "         history.append({\"role\": \"user\", \"content\": user_message}) # Add user msg to actual history\n",
        "\n",
        "\n",
        "    # Reasoning: Format the history (including the latest user message) into the model-specific prompt string.\n",
        "    prompt = format_prompt_mistral(current_turn_history)\n",
        "\n",
        "    # Reasoning: Determine the device the model is currently loaded on (e.g., 'cuda:0').\n",
        "    # This ensures input tensors are moved to the same device as the model.\n",
        "    try:\n",
        "         current_device = next(model_to_use.parameters()).device\n",
        "    except Exception as e:\n",
        "         print(f\"❌ Error detecting model device: {e}. Assuming CPU.\")\n",
        "         current_device = 'cpu'\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Reasoning: Convert the formatted prompt string into numerical tokens using the tokenizer.\n",
        "        # return_tensors=\"pt\": Return PyTorch tensors.\n",
        "        # .to(current_device): Move the tensors to the GPU (or CPU if model is there).\n",
        "        model_inputs = tokenizer_to_use(prompt, return_tensors=\"pt\").to(current_device)\n",
        "\n",
        "        # --- Generation Parameters ---\n",
        "        # Reasoning: Configure how the model generates the response text.\n",
        "        generation_config = {\n",
        "            \"max_new_tokens\": 250,          # Max tokens to generate for the response. Adjust based on desired length.\n",
        "            \"pad_token_id\": tokenizer_to_use.eos_token_id, # Use EOS token for padding during generation.\n",
        "            \"do_sample\": True,             # Enable sampling for less deterministic, more natural responses.\n",
        "            \"temperature\": 0.6,            # Controls randomness. Lower = more focused/deterministic, Higher = more random. (0.6 is slightly conservative).\n",
        "            \"top_p\": 0.9,                  # Nucleus sampling: consider only the most probable tokens whose cumulative probability exceeds top_p. (0.9 is common).\n",
        "            # \"top_k\": 50,                 # Optional: Alternative sampling: consider only the top_k most likely tokens. Often used with or instead of top_p.\n",
        "            # \"repetition_penalty\": 1.1    # Optional: Penalize repeating tokens slightly (values > 1).\n",
        "        }\n",
        "\n",
        "        print(f\"   Generating response with config: {generation_config}\")\n",
        "        start_time = time.time()\n",
        "        # Reasoning: Disable gradient calculations during inference. This saves memory and speeds up generation.\n",
        "        with torch.no_grad():\n",
        "             # Reasoning: Call the model's 'generate' method with the tokenized input and generation parameters.\n",
        "             generated_ids = model_to_use.generate(**model_inputs, **generation_config)\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        # Reasoning: Decode the generated token IDs back into a string.\n",
        "        # Important: Slice generated_ids to remove the input prompt tokens, keeping only the newly generated response tokens.\n",
        "        # [0] accesses the first (and usually only) sequence in the batch.\n",
        "        response_ids = generated_ids[0][model_inputs[\"input_ids\"].shape[1]:]\n",
        "        assistant_response = tokenizer_to_use.decode(response_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        print(f\"   ✅ LLM Response ({duration:.2f}s): '{assistant_response}'\")\n",
        "\n",
        "        # Reasoning: Add the successfully generated assistant response to the *persistent* history list for future context.\n",
        "        history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        return assistant_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during LLM generation or decoding: {e}\")\n",
        "        # Reasoning: Implement recovery logic. If generation fails, remove the last user message and the potentially failed\n",
        "        # assistant message from the persistent history to prevent the error from corrupting future turns.\n",
        "        if history:\n",
        "            if history[-1][\"role\"] == \"user\":\n",
        "                history.pop() # Remove the user message that caused the error\n",
        "            elif history[-1][\"role\"] == \"assistant\":\n",
        "                # This case is less likely but possible if error happens after adding assistant response\n",
        "                history.pop() # Remove failed assistant response\n",
        "                if history and history[-1][\"role\"] == \"user\":\n",
        "                     history.pop() # Remove triggering user message too\n",
        "\n",
        "\n",
        "        return \"My apologies, I encountered an unexpected issue while generating a response. Could you please rephrase or try again?\"\n",
        "\n",
        "print(\"\\n✅ 'Think' function defined (get_llm_response) with history management.\")\n",
        "# You can uncomment the example usage lines in the previous version's Step 6 cell to test the base model now if desired.\n",
        "\n",
        "# Example Usage (Optional - uncomment to test the base model)\n",
        "# print(\"\\n--- Testing LLM Response (Base Model) ---\")\n",
        "# if llm_model and llm_tokenizer:\n",
        "#      test_resp = get_llm_response(\"What are the main stages of sleep?\", llm_model, llm_tokenizer, conversation_history_base)\n",
        "#      print(f\"\\nBase Model Response: {test_resp}\")\n",
        "#      print(f\"\\nBase History: {conversation_history_base}\")\n",
        "# else:\n",
        "#      print(\"\\nCannot test LLM, model or tokenizer not loaded.\")\n",
        "# print(\"--- End Test ---\")\n",
        "\n",
        "# @title Step 7: Define \"Speak\" Function (TTS Audio Synthesis & Display)\n",
        "# Reasoning: Defines the function that takes text, converts it to speech using the loaded Piper TTS model,\n",
        "# saves the audio, and makes it playable within the Colab notebook output.\n",
        "\n",
        "from IPython.display import Audio, display # For embedding audio player in Colab.\n",
        "import soundfile as sf # For writing audio data to WAV files.\n",
        "import numpy as np # For handling audio data arrays.\n",
        "import time # For timing the synthesis process.\n",
        "from pathlib import Path # For path manipulation.\n",
        "import os # For creating directories.\n",
        "\n",
        "# --- Output Directory Setup ---\n",
        "# Reasoning: Create a dedicated directory to store the generated audio responses.\n",
        "# This keeps the Colab filesystem organized.\n",
        "audio_output_dir = Path(\"./agent_audio_output\")\n",
        "audio_output_dir.mkdir(exist_ok=True) # Create it if it doesn't exist.\n",
        "# Reasoning: Use a simple counter to give each saved audio file a unique name.\n",
        "output_audio_counter = 0\n",
        "\n",
        "# --- Text-to-Speech Function ---\n",
        "def speak_text_and_display(text_to_speak):\n",
        "    \"\"\"\n",
        "    Synthesizes text using Piper TTS, saves as WAV, and displays playable audio in Colab.\n",
        "\n",
        "    Args:\n",
        "        text_to_speak (str): The text the agent should say.\n",
        "    \"\"\"\n",
        "    global output_audio_counter\n",
        "    # Reasoning: Check if the TTS model object is loaded and ready.\n",
        "    if not tts_voice:\n",
        "        print(\"❌ Error: Piper TTS voice ('tts_voice') is not loaded.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n🔊 Synthesizing speech...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # Reasoning: Use the synthesize_stream_raw method which yields chunks of raw audio bytes (PCM data).\n",
        "        # Iterate through these chunks and concatenate them into a single NumPy array.\n",
        "        # Specify dtype=np.int16 as this is common for Piper's PCM output.\n",
        "        audio_data = np.concatenate([\n",
        "            np.frombuffer(chunk, dtype=np.int16)\n",
        "            for chunk in tts_voice.synthesize_stream_raw(text_to_speak)\n",
        "        ])\n",
        "\n",
        "        # Reasoning: Handle cases where synthesis might produce no audio (e.g., empty input text).\n",
        "        if audio_data.size == 0:\n",
        "            print(\"   ⚠️ Warning: TTS synthesis resulted in empty audio data.\")\n",
        "            return\n",
        "\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        print(f\"   (Audio synthesis took {duration:.2f} seconds)\")\n",
        "\n",
        "        # --- Save Audio File ---\n",
        "        # Reasoning: Construct a unique filename using the counter for the WAV file.\n",
        "        output_filename = audio_output_dir / f\"napcoach_response_{output_audio_counter}.wav\"\n",
        "        output_audio_counter += 1\n",
        "        # Reasoning: Use the soundfile library (sf.write) to save the NumPy audio data array\n",
        "        # to a WAV file. It's crucial to provide the correct 'tts_sample_rate' obtained during TTS model loading.\n",
        "        sf.write(output_filename, audio_data, tts_sample_rate)\n",
        "        print(f\"   💾 Audio saved to: {output_filename}\")\n",
        "\n",
        "        # --- Display Audio Player in Colab ---\n",
        "        # Reasoning: Use IPython.display.Audio to embed an HTML5 audio player\n",
        "        # directly into the Colab cell's output, allowing playback of the saved WAV file.\n",
        "        # autoplay=False is generally preferred to avoid unexpected sound.\n",
        "        display(Audio(str(output_filename), autoplay=False))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during TTS synthesis or saving/displaying audio: {e}\")\n",
        "\n",
        "print(\"\\n✅ 'Speak' function defined (speak_text_and_display).\")\n",
        "# You can uncomment the example usage lines in the previous version's Step 7 cell to test TTS now if desired.\n",
        "\n",
        "# Example Usage (Optional - uncomment to test)\n",
        "# print(\"\\n--- Testing TTS ---\")\n",
        "# speak_text_and_display(\"Hello! This is your sleep coach speaking.\")\n",
        "# if 'test_resp' in locals() and test_resp:\n",
        "#      speak_text_and_display(test_resp)\n",
        "# print(\"--- End Test ---\")\n",
        "\n",
        "# @title Define Gradio-Specific TTS Function (Place after your original Step 7)\n",
        "\n",
        "from pathlib import Path # Ensure Path is imported\n",
        "import numpy as np    # Ensure numpy is imported\n",
        "import soundfile as sf  # Ensure soundfile is imported\n",
        "import re             # Ensure re is imported\n",
        "import os             # Ensure os is imported\n",
        "import time           # Ensure time is imported\n",
        "\n",
        "# This directory will store TTS outputs for Gradio to serve\n",
        "GRADIO_TTS_OUTPUT_DIR = Path(\"./gradio_tts_audio_responses\")\n",
        "os.makedirs(GRADIO_TTS_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def synthesize_for_gradio_and_autoplay(text_to_speak: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Synthesizes speech, saves to a file, and returns the filepath.\n",
        "    Gradio's Audio component with autoplay=True will play it.\n",
        "    \"\"\"\n",
        "    global tts_voice, tts_sample_rate # Assumes these are globally loaded from your setup\n",
        "\n",
        "    if not text_to_speak:\n",
        "        print(\"Gradio TTS: No text to synthesize.\")\n",
        "        return None\n",
        "    if tts_voice is None:\n",
        "        print(\"❌ Gradio TTS: TTS Voice not loaded!\")\n",
        "        return None\n",
        "\n",
        "    timestamp = int(time.time() * 1000)\n",
        "    # Save directly into a location Gradio can serve if needed, or just a temp path.\n",
        "    # For simplicity, we'll save it and Gradio will make it accessible.\n",
        "    output_filename = f\"napcoach_gradio_response_{timestamp}.wav\"\n",
        "    output_filepath = GRADIO_TTS_OUTPUT_DIR / output_filename\n",
        "\n",
        "    try:\n",
        "        print(f\"Gradio TTS: Synthesizing '{text_to_speak}' to {output_filepath}...\")\n",
        "\n",
        "        # --- TTS Pre-processing for dashes ---\n",
        "        processed_tts_text = text_to_speak\n",
        "        processed_tts_text = re.sub(r'(\\d+)-(\\d+)', r'\\1 to \\2', processed_tts_text)\n",
        "        # Add any other pre-processing you need here\n",
        "        # --- End TTS pre-processing ---\n",
        "\n",
        "        audio_data_chunks = []\n",
        "        for chunk in tts_voice.synthesize_stream_raw(processed_tts_text):\n",
        "            audio_data_chunks.append(np.frombuffer(chunk, dtype=np.int16))\n",
        "\n",
        "        if not audio_data_chunks:\n",
        "            print(\"Gradio TTS: No audio data generated from synthesize_stream_raw.\")\n",
        "            return None\n",
        "\n",
        "        audio_data = np.concatenate(audio_data_chunks)\n",
        "        if audio_data.size == 0:\n",
        "            print(\"Gradio TTS: Concatenated audio data is empty.\")\n",
        "            return None\n",
        "\n",
        "        sf.write(str(output_filepath), audio_data, tts_sample_rate)\n",
        "        print(f\"Gradio TTS: Audio saved to {output_filepath}\")\n",
        "        return str(output_filepath) # Return the filepath as a string\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gradio TTS Error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"✅ Gradio-specific TTS function 'synthesize_for_gradio_and_autoplay' defined.\")\n",
        "\n",
        "# @title Step 8: Prepare and Load Fine-Tuning Dataset (Using User-Provided Data)\n",
        "# Reasoning: This step defines, saves, and loads the specialized dataset used to adapt the LLM\n",
        "# for sleep coaching conversations, incorporating wearable data, diaries, and research knowledge.\n",
        "# This version uses the comprehensive dataset provided by the user.\n",
        "\n",
        "from datasets import load_dataset, Dataset, Features, Value\n",
        "import json\n",
        "import os\n",
        "\n",
        "# --- Define Fine-Tuning Data (User-Provided Examples) ---\n",
        "# Reasoning: Use the high-quality, diverse examples provided by the user, covering wearables,\n",
        "# diaries, research Q&A, and safety boundaries. Store as a list of dictionaries.\n",
        "fine_tuning_examples = [\n",
        "    {\"instruction\": \"User data: Slept 5 hours 10 minutes, 45 mins deep sleep, 1 hour REM. Felt very tired waking up. Question: What should I focus on?\", \"response\": \"Getting only around 5 hours isn't ideal for most adults, which likely explains the tiredness. Focus on extending your total sleep time towards the 7-9 hour range. A consistent wind-down routine before bed might help you fall asleep faster and get more restorative deep and REM sleep.\"},\n",
        "    {\"instruction\": \"What's the ideal temperature for a bedroom for good sleep?\", \"response\": \"Research suggests a cool room, typically between 60-67 degrees Fahrenheit (15-19 Celsius), is optimal for sleep. This helps lower your core body temperature, signaling it's time to rest.\"},\n",
        "    {\"instruction\": \"I woke up frequently last night, maybe 4-5 times according to my tracker. Is this normal?\", \"response\": \"Brief awakenings during the night are normal and often forgotten. However, waking up 4-5 times and remembering it might indicate fragmented sleep. Consider factors like noise, light, temperature, needing the bathroom, or stress. If it persists, keeping a sleep diary or consulting a professional could be helpful.\"},\n",
        "    {\"instruction\": \"Analyze this: Bedtime 11:30 PM, Wake time 7:00 AM. Time in bed 7.5 hours. Total Sleep: 6 hours 45 mins. Sleep Efficiency: 90%. Deep: 1h 15m, REM: 1h 30m. Feeling: Okay.\", \"response\": \"Your sleep efficiency of 90% is good (above 85% is generally considered fine). Total sleep time is near the lower end of the recommended 7-9 hours, which might explain feeling just 'Okay'. You have healthy amounts of Deep and REM sleep within that time. If you want to feel more rested, you could try shifting your bedtime slightly earlier to increase total sleep duration.\"},\n",
        "    {\"instruction\": \"My sleep tracker says I only got 30 minutes of deep sleep last night out of 7 hours total. Is that bad?\", \"response\": \"While individual needs vary, typically deep sleep makes up about 13-23% of total sleep for adults. 30 minutes out of 7 hours is roughly 7%, which is on the low side. Factors like stress, alcohol before bed, or an uncomfortable sleep environment can reduce deep sleep. Focusing on a consistent bedtime routine and managing stress might help increase it.\"},\n",
        "    {\"instruction\": \"I went to bed at 10 PM but didn't fall asleep until after midnight. What can I do?\", \"response\": \"Lying in bed awake for that long can create frustration. If you don't fall asleep within about 20-30 minutes, it's often recommended to get out of bed, do a relaxing, non-stimulating activity (like reading a physical book under dim light) in another room, and only return to bed when you feel sleepy again. Also, review your pre-sleep routine – avoid screens and caffeine late in the day.\"},\n",
        "    {\"instruction\": \"Is it better to sleep cold or warm?\", \"response\": \"Generally, a cooler room temperature is better for sleep, typically around 60-67 degrees Fahrenheit (15-19 Celsius). A cool environment helps lower your core body temperature, which signals to your body that it's time to sleep. Sleeping too warm can lead to restlessness and fragmented sleep.\"},\n",
        "    {\"instruction\": \"What's sleep hygiene?\", \"response\": \"Sleep hygiene refers to habits and practices that help promote consistently good sleep. Key elements include maintaining a regular sleep-wake schedule (even on weekends), creating a relaxing bedtime routine, ensuring your bedroom is dark, quiet, and cool, avoiding large meals, caffeine, and alcohol close to bedtime, and getting regular exercise (but not too close to bedtime).\"},\n",
        "    {\"instruction\": \"My Whoop report: 7h 10m Time in Bed, 6h 35m Sleep, 89% Efficiency. Stages: 15% Awake, 25% REM, 45% Light, 15% Deep. RHR 55bpm, HRV 68ms. Felt pretty good waking up. Is this a good sleep profile?\", \"response\": \"Based on your report, that looks like a solid night! An efficiency of 89% is great (above 85% is generally good). 6h 35m of sleep is within the healthy range for many adults, though some need more. Your sleep stage percentages (25% REM, 15% Deep) are well within typical healthy ranges. The RHR of 55 and HRV of 68ms also sound positive, reflecting good recovery. Most importantly, you felt good waking up, which is a key indicator!\"},\n",
        "    {\"instruction\": \"Fitbit stats: TIB 8h 30m, TST 7h 05m, Efficiency 83%. Stages: Awake 1h 25m (17%), REM 1h 15m (18%), Light 4h 30m (64%), Deep 55m (13%). RHR 60bpm. I feel tired and sluggish today. Why?\", \"response\": \"Thanks for sharing. While 7h 05m total sleep isn't drastically low, the lower efficiency (83%) and significant time awake (nearly 1.5 hours) suggest your sleep might have been fragmented. Also, your Deep sleep percentage (13%) is right at the lower edge of the typical 13-23% range. Deep sleep is vital for physical restoration, so getting less could contribute to feeling tired and sluggish. Focusing on minimizing nighttime disturbances and optimizing your sleep environment might help improve both efficiency and deep sleep.\"},\n",
        "    {\"instruction\": \"Apple Health Sleep shows only 45 minutes of Deep sleep out of 7.5 hours total sleep. Is this something to worry about?\", \"response\": \"Deep sleep typically makes up about 13-23% of total sleep for adults, so 45 minutes out of 7.5 hours (which is about 10%) is indeed on the lower side. While occasional nights with less deep sleep happen, consistently getting low amounts might leave you feeling less physically refreshed. Factors like stress, alcohol consumption before bed, sleeping too warm, or irregular sleep schedules can reduce deep sleep. Improving sleep hygiene often helps. However, if you consistently feel unrefreshed or have concerns, it's always best to consult with a healthcare professional for personalized advice.\"},\n",
        "    {\"instruction\": \"My Garmin recorded a Resting Heart Rate of 65bpm during sleep, usually it's around 55bpm. What could cause this increase?\", \"response\": \"Resting Heart Rate (RHR) during sleep can fluctuate based on various factors. An occasional increase like going from 55 to 65bpm could be due to things like increased stress the previous day, eating a heavy meal or drinking alcohol close to bedtime, fighting off a mild illness (even before you feel sick), or even just a warmer sleeping environment. Look for trends over several days rather than focusing on a single night. If your RHR stays elevated consistently or if you have other symptoms or concerns, checking in with a healthcare professional is recommended.\"},\n",
        "    {\"instruction\": \"What does Heart Rate Variability (HRV) during sleep mean? Mine was 55ms.\", \"response\": \"Heart Rate Variability (HRV) measures the variation in time between consecutive heartbeats. Higher HRV during sleep is generally associated with better rest, recovery, and nervous system balance (specifically, more parasympathetic or 'rest-and-digest' activity). 55ms is a specific value, and what's 'good' is highly individual and depends on your baseline, age, fitness, etc. It's more useful to track your *trend* over time rather than comparing to others. A consistent rise might indicate improved recovery, while a significant drop could suggest increased stress, overtraining, or illness.\"},\n",
        "    {\"instruction\": \"Sleep Diary: Bed 10:30 PM, intended sleep 11:00 PM. Read book, lights out 11:10 PM. Mind felt busy, tossed around. Think I fell asleep around 12:45 AM. Woke up ~4 AM, couldn't get back sleep until 5:15 AM. Alarm 6:30 AM. Feel wrecked. Had a stressful work day.\", \"response\": \"That sounds like a really rough night, and it makes sense you feel wrecked. The diary clearly shows significant difficulty falling asleep (over 1.5 hours after lights out) and a long awakening mid-night (over an hour). High stress from your workday very likely contributed to your mind racing and made it hard to initiate and maintain sleep. When this happens, standard advice includes having a consistent wind-down routine specifically designed to de-stress before bed (e.g., meditation, gentle stretching, journaling). If high stress frequently impacts your sleep, exploring stress management techniques during the day might also be beneficial.\"},\n",
        "    {\"instruction\": \"Diary: Asleep 11:30 PM, woke 7:30 AM feeling amazing! Slept soundly. Odd thing: I skipped dinner last night because I worked late.\", \"response\": \"It's fantastic you felt amazing after sleeping soundly for 8 hours! While skipping dinner isn't generally recommended as a regular habit for overall health and energy levels, it might not have negatively impacted your sleep *this specific night*, especially if you weren't overly hungry. Sometimes, a very large or heavy meal close to bed can disrupt sleep more than skipping occasionally might. However, consistently skipping meals can affect energy and potentially sleep patterns long-term, so it's good you noticed it was unusual.\"},\n",
        "    {\"instruction\": \"I wake up at 4 AM almost every single morning, wide awake, and can't get back to sleep. I go to bed around 11 PM and need to wake at 7 AM. Help!\", \"response\": \"Waking up too early and being unable to return to sleep (known as early morning awakening insomnia) is really disruptive. Potential contributing factors include stress, anxiety, your natural circadian rhythm shifting earlier (common as we age), or even light exposure too early. Some strategies include: 1. Ensuring your bedroom is completely dark (use blackout curtains if needed). 2. Avoid checking the time when you wake up. 3. Stick to a very consistent bedtime. 4. Consider if you're going to bed *too* early for your natural rhythm. 5. If you don't fall back asleep after 15-20 mins, get up and do a quiet activity until sleepy. If this persists despite trying these, consulting a healthcare professional is important to rule out other causes.\"},\n",
        "    {\"instruction\": \"Can you explain the different stages of sleep simply?\", \"response\": \"Think of sleep happening in cycles. Each cycle has stages: First, you drift into LIGHT sleep (Stages N1 and N2), where your body starts relaxing. Then, you enter DEEP sleep (Stage N3 or Slow-Wave Sleep), which is crucial for physical repair, growth, and feeling refreshed. After deep sleep, you typically go into REM (Rapid Eye Movement) sleep, where most dreaming occurs, and which is vital for memory consolidation, learning, and emotional processing. You cycle through these stages multiple times a night, with more deep sleep earlier and more REM sleep later.\"},\n",
        "    {\"instruction\": \"What's the most important part of sleep hygiene?\", \"response\": \"While all aspects of sleep hygiene are helpful, arguably the MOST important element for many people is maintaining a **consistent sleep-wake schedule**. Going to bed and waking up around the same time every day, even on weekends, helps regulate your body's internal clock (circadian rhythm). This makes it easier to fall asleep at night and wake up feeling refreshed naturally. Other key elements like creating a cool, dark, quiet environment and having a relaxing bedtime routine are also very important, but the schedule often provides the strongest foundation.\"},\n",
        "    {\"instruction\": \"Is it bad to use my phone in bed before trying to sleep?\", \"response\": \"Yes, it's generally advised against using your phone (or other bright screens like tablets/laptops) in bed before sleep. The blue light emitted suppresses melatonin production, making it harder to feel sleepy. Additionally, the content you engage with (social media, news, games) can be mentally stimulating or even stressful, further hindering your ability to wind down. It's best to put screens away at least 30-60 minutes before bed and keep the bedroom as a screen-free zone associated with sleep.\"},\n",
        "    {\"instruction\": \"I feel sleepy around 3 PM every day. Should I take a nap?\", \"response\": \"That afternoon dip in alertness is quite common due to our natural circadian rhythm. Whether to nap depends on your situation. A short 'power nap' of about 20-30 minutes can effectively boost alertness without causing grogginess (sleep inertia) or significantly impacting your nighttime sleep. However, if you have trouble falling asleep or staying asleep at night (insomnia), it's often recommended to avoid napping altogether, as it can reduce your 'sleep drive' for nighttime sleep. If you do nap, keep it short and avoid napping too late in the day (e.g., after 4 PM).\"},\n",
        "    {\"instruction\": \"Does eating close to bedtime affect sleep?\", \"response\": \"Yes, eating a large or heavy meal close to bedtime can negatively affect sleep. Digestion is an active process that can raise your body temperature slightly and potentially cause discomfort like indigestion or heartburn, making it harder to fall asleep or stay asleep. It's generally recommended to finish dinner at least 2-3 hours before your intended bedtime. If you're hungry later, a small, light, easily digestible snack might be okay, but avoid heavy, fatty, or spicy foods.\"},\n",
        "    {\"instruction\": \"I think I might have sleep apnea, I snore loudly and sometimes wake up choking.\", \"response\": \"For medical concerns or diagnoses, it's essential to consult with a qualified healthcare professional. Snoring loudly and waking up choking or gasping can be signs of sleep apnea, which requires proper medical evaluation and treatment.\"},\n",
        "    {\"instruction\": \"Explain why a cool room helps sleep.\", \"response\": \"A cool room helps facilitate sleep because our core body temperature naturally needs to drop slightly to initiate and maintain sleep. A cooler environment (typically 60-67°F or 15-19°C) supports this natural temperature drop, signaling to your body that it's time to rest. A room that's too warm can interfere with this process, leading to restlessness, more awakenings, and less time spent in restorative deep and REM sleep.\"},\n",
        "    {\"instruction\": \"My sleep tracker often shows less 'Total Sleep Time' than my 'Time in Bed'. Why the difference?\", \"response\": \"That's completely normal! 'Time in Bed' is simply the total duration from when you got into bed until you got out of bed. 'Total Sleep Time' is the estimate of how much of that time you were actually asleep. The difference accounts for the time it took you to fall asleep initially (sleep latency), any time you spent awake during the night (whether you remember it or not), and time spent in bed after waking up in the morning. Higher 'Sleep Efficiency' (Total Sleep Time divided by Time in Bed) generally indicates better sleep quality.\"},\n",
        "    {\"instruction\": \"Is 'Light Sleep' important? My tracker says I get a lot of it.\", \"response\": \"Yes, light sleep (which includes stages N1 and N2) is important and typically makes up the largest portion of total sleep time (around 50-60% or more). While N1 is very light (the transition phase), N2 is where you spend much of your sleep. During N2, your body continues to relax, body temperature drops, and brain waves slow down. It plays a role in memory consolidation and processing, and it serves as the pathway to deeper sleep stages and REM. So, getting plenty of light sleep is a normal and necessary part of a healthy sleep cycle.\"},\n",
        "    {\"instruction\": \"User Data: TST: 5h 30m, Efficiency 75%, Deep 1h (18%), REM 1h (18%), RHR 50. Feeling surprisingly okay. Is this sustainable?\", \"response\": \"It's interesting that you feel okay despite getting only 5.5 hours of sleep, which is significantly less than the recommended 7-9 hours for most adults. While your Deep and REM percentages within that sleep time look reasonable, and your RHR is low, consistently getting this little sleep is generally not considered sustainable or optimal for long-term physical and cognitive health, even if you feel functional in the short term. Chronic partial sleep deprivation can have cumulative negative effects. It might be worth exploring strategies to gradually extend your sleep duration closer to the recommended range.\"},\n",
        "    {\"instruction\": \"Can meditation help me sleep better?\", \"response\": \"Yes, meditation and mindfulness practices can be very effective tools for improving sleep for many people. They work by helping to calm the mind, reduce racing thoughts, lower stress and anxiety levels, and activate the body's relaxation response (parasympathetic nervous system). Practicing regularly, especially as part of a wind-down routine before bed, can make it easier to fall asleep and potentially improve sleep quality. There are many guided meditations specifically designed for sleep available through apps or online.\"},\n",
        "    {\"instruction\": \"My Whoop recovery score is low (35%) even though I slept 8 hours. What gives?\", \"response\": \"A low recovery score on Whoop despite adequate sleep duration often suggests that while you slept long enough, the *quality* of that sleep might not have been optimal for recovery, or other factors influenced the score. Key metrics impacting Whoop recovery include HRV, RHR, and Respiratory Rate, compared to your personal baseline. A low score could indicate higher-than-usual RHR, lower-than-usual HRV, or elevated respiratory rate during sleep. This might happen due to factors like high stress the previous day, intense late-night exercise, illness, alcohol consumption, poor sleep environment, or even just natural variation. Look at the specific metric trends within the app for more clues.\"},\n",
        "    {\"instruction\": \"Diary: Fell asleep around 11pm, woke up 6am. But I remember dreaming ALL night, really vivid dreams. Felt a bit mentally tired after. Normal?\", \"response\": \"Remembering vivid dreams often happens when you wake up directly from or soon after REM sleep. REM sleep is most concentrated in the later part of the night, so waking up around 6am after falling asleep at 11pm means you likely had significant REM periods. While dreaming is normal and healthy (part of REM's function), feeling mentally tired *despite* adequate sleep duration might occasionally happen after nights with unusually intense or prolonged REM, though it's often linked to other factors too. If it's just an occasional occurrence and you generally feel okay, it's likely normal variation in your sleep cycles.\"},\n",
        "    {\"instruction\": \"What's the connection between stress and sleep?\", \"response\": \"Stress and sleep have a strong, two-way relationship. High stress levels activate the body's 'fight-or-flight' response, releasing hormones like cortisol and adrenaline. These hormones increase alertness and physiological arousal, making it difficult to fall asleep and stay asleep (causing insomnia). Conversely, poor sleep or lack of sleep increases stress hormone levels, reduces emotional resilience, and makes you more susceptible to feeling stressed during the day. Managing stress through techniques like exercise, mindfulness, and relaxation is therefore crucial for improving sleep, and getting good sleep is vital for managing stress.\"},\n",
        "    {\"instruction\": \"Is it better to stick to the same bedtime or the same wake-up time?\", \"response\": \"While both are helpful, sleep experts generally emphasize that sticking to a **consistent wake-up time** is more critical for anchoring your body's internal clock (circadian rhythm). Waking up at the same time every day, even on weekends, helps regulate your sleep-wake cycle most effectively. Your bedtime might naturally vary slightly based on when you feel tired, but the consistent wake-up time provides the strongest signal for maintaining rhythm.\"},\n",
        "    {\"instruction\": \"Explain 'sleep pressure' or 'sleep drive'.\", \"response\": \"Sleep pressure, also called sleep drive or homeostatic sleep drive, is the natural build-up of sleepiness that occurs the longer you stay awake. Think of it like hunger – the longer you go without eating, the hungrier you get. Similarly, the longer you're awake, the more a substance called adenosine accumulates in your brain, increasing the 'pressure' to sleep. Getting sufficient sleep resets this pressure. Understanding sleep pressure helps explain why naps can sometimes make nighttime sleep harder (they reduce the pressure) and why maintaining a consistent schedule helps align sleep pressure with your circadian rhythm.\"},\n",
        "    {\"instruction\": \"My partner says I stopped breathing momentarily in my sleep. What should I do?\", \"response\": \"For medical concerns or diagnoses, it's essential to consult with a qualified healthcare professional. Pauses in breathing during sleep can be a sign of sleep apnea, a condition that needs proper medical evaluation and management. Please schedule an appointment with your doctor to discuss what your partner observed.\"},\n",
        "    {\"instruction\": \"Can white noise help with sleep?\", \"response\": \"White noise can be helpful for some people, especially those sensitive to background noise or living in noisy environments. It works by creating a constant, consistent soundscape that masks other, more abrupt or jarring sounds (like traffic, doors closing, or a partner snoring) that might otherwise wake you up or prevent you from falling asleep. However, it's not beneficial for everyone, and some prefer complete quiet. If you try it, keep the volume at a reasonable level.\"},\n",
        "    {\"instruction\": \"Fitbit Sleep Score was 75 ('Fair'). TST 6h 50m, Deep 15%, REM 20%. Why just 'Fair'?\", \"response\": \"A Fitbit Sleep Score considers multiple factors beyond just duration. A score of 75 ('Fair') likely means some aspects were good, while others could be improved. While your Deep (15%) and REM (20%) percentages are within typical ranges, the Total Sleep Time (6h 50m) is slightly below the commonly recommended 7+ hours. Fitbit also heavily weights 'Restoration' metrics, which look at your sleeping heart rate compared to your resting heart rate and how much time you spent below RHR. Perhaps your sleeping heart rate wasn't as low as usual, or you had periods of restlessness (counted in the 'Awake/Restless' time). Check the detailed breakdown in the Fitbit app for more specific insights on why the score wasn't higher.\"},\n",
        "    {\"instruction\": \"I drank coffee at 4 PM yesterday and couldn't fall asleep until 1 AM. Is that related?\", \"response\": \"Yes, that's very likely related. Caffeine has an average half-life of about 5-6 hours, meaning half the caffeine is still in your system that long after consumption. Drinking coffee at 4 PM means a significant amount was likely still active in your brain when you tried to sleep around 11 PM or midnight, blocking adenosine receptors and promoting alertness. This commonly causes difficulty falling asleep (increased sleep latency). To avoid this, it's generally recommended to stop caffeine intake at least 6-8 hours before your intended bedtime.\"},\n",
        "    {\"instruction\": \"Whoop: TST 6h 55m, Eff 85%, Awake 1h 10m (14%), REM 28%, Light 42%, Deep 16%. RHR 57, HRV 58ms. Diary: Felt pretty stressed yesterday, did a light yoga routine before bed. Woke up feeling decent, not amazing.\", \"response\": \"Getting almost 7 hours of sleep with 85% efficiency is quite good, especially considering you were stressed yesterday. Your REM sleep (28%) is excellent, suggesting good cognitive processing, and Deep sleep (16%) is within the normal range. The RHR (57) and HRV (58ms) seem reasonable, though perhaps not your personal best if stress was high. The light yoga was a good choice for wind-down. Feeling 'decent' rather than 'amazing' could simply be the lingering effects of yesterday's stress, even if sleep metrics look mostly okay. Consistent stress management and sleep routines should help.\"},\n",
        "    {\"instruction\": \"Fitbit: TIB 8h 40m, TST 7h 50m, Eff 90%. Stages: Deep 10%, REM 19%. RHR 60. Question: My Deep and REM % seem low even though I slept almost 8 hours with good efficiency. What gives?\", \"response\": \"That's a valid observation. While your overall duration (7h 50m) and efficiency (90%) are excellent, having both Deep sleep at 10% (below 13-23% typical range) and REM at 19% (at the very low end of 20-25% typical range) suggests the *architecture* of your sleep might be suboptimal, even if it was continuous. Factors that can selectively suppress these stages include alcohol consumption, certain medications, inconsistent sleep schedules (even if total time is long), or underlying stress/health issues. It might be worth reviewing your pre-sleep habits and environment for potential disruptors specifically affecting these restorative stages.\"},\n",
        "    {\"instruction\": \"Apple Watch: TST 7h 00m. Respiratory Rate during sleep averaged 16 breaths/min, usually it's 14. Should I worry?\", \"response\": \"A small increase in average Respiratory Rate during sleep, like from 14 to 16 breaths per minute, isn't usually a cause for immediate concern on its own, as it can fluctuate slightly. Potential minor causes include nasal congestion (making breathing slightly harder), sleeping in a warmer room, recent strenuous exercise, or even changes in dream activity. However, consistently elevated respiratory rate, especially if accompanied by other symptoms like loud snoring, gasping, excessive daytime sleepiness, or drops in blood oxygen (if measured), could potentially indicate underlying issues. For medical concerns or diagnoses, it's essential to consult with a qualified healthcare professional.\"},\n",
        "    {\"instruction\": \"Garmin Body Battery started at 60 yesterday, drained to 5 by bedtime, and only recharged to 55 overnight despite 7h TST. Why such poor recharge?\", \"response\": \"Garmin's Body Battery estimates your energy reserves based on HRV, stress, and activity. Recharging primarily happens during restful sleep. Only recharging to 55 despite 7 hours TST suggests the sleep wasn't as restorative as it could have been, or daytime stress/activity significantly depleted your reserves. Factors contributing to poor recharge include: high stress levels carrying into sleep (check overnight stress graph if available), poor sleep quality (even if duration seems okay - look at HRV/RHR trends during sleep), consuming alcohol, being unwell, or over-training. The low starting point (drain to 5) also means more recovery was needed. Focus on both stress management during the day and optimizing sleep quality at night.\"},\n",
        "    {\"instruction\": \"Oura Ring shows my body temperature deviated +0.5 degrees Celsius above baseline last night. I feel fine though.\", \"response\": \"Oura tracks subtle body temperature changes during sleep. A deviation of +0.5°C (about +0.9°F) above your baseline is a noticeable increase. While you feel fine now, this often indicates your body is fighting something off – it can be an early sign of illness even before other symptoms appear. It could also be related to factors like intense exercise late in the day, a large meal close to bed, alcohol, or hormonal cycles (in women). Keep an eye on how you feel over the next day or two, and monitor the temperature trend. If it stays elevated or you develop symptoms, prioritize rest.\"},\n",
        "    {\"instruction\": \"Diary: Bed 11pm, Asleep 11:20pm. Woke 6:45am. Watch Data: TST 7h 30m, Eff 95%, Deep 20%, REM 25%, RHR 51, HRV 70. Feeling: STILL TIRED! How is this possible?\", \"response\": \"That's frustrating! Objectively, your sleep data looks fantastic – excellent duration, efficiency, stage balance, and recovery metrics (RHR/HRV). Feeling tired despite this can sometimes happen due to: 1. **Sleep Inertia:** Significant grogginess immediately upon waking that might take longer than usual to shake off. 2. **Accumulated Sleep Debt:** If you were significantly sleep-deprived on previous nights, one good night might not be enough to fully recover. 3. **Underlying Health Factors:** Things unrelated to sleep quality itself (like nutrient deficiencies, thyroid issues, chronic fatigue syndrome, low-grade illness) can cause daytime tiredness. 4. **Circadian Misalignment:** Even if duration is good, perhaps the timing isn't quite right for your internal clock. Given the excellent metrics, if this persists, exploring non-sleep factors with a healthcare professional might be warranted.\"},\n",
        "    {\"instruction\": \"Diary: Stayed up until 2 AM studying for an exam, slept until 7 AM (5 hours). Have another exam today. Best strategy now? Nap or caffeine?\", \"response\": \"Getting only 5 hours of sleep before an exam is tough. Both napping and caffeine have pros and cons here. **Caffeine** can provide a temporary boost in alertness for the exam but might lead to a crash later and won't address the underlying sleep debt. **A short nap** (20-30 minutes) could improve alertness and potentially memory consolidation without causing too much grogginess, but finding time might be hard, and oversleeping could make you feel worse. **Strategy:** If possible, a short power nap earlier in the day (e.g., early afternoon) might be most beneficial for cognitive function. If napping isn't feasible, use caffeine strategically (e.g., one dose before the exam), stay hydrated, and prioritize getting recovery sleep *after* your exam today.\"},\n",
        "    {\"instruction\": \"Had a nightmare that woke me up around 4 AM. Heart was pounding. Took me an hour to get back to sleep. Impact?\", \"response\": \"Nightmares, especially vivid ones that wake you up with a physical stress response (pounding heart), can definitely impact sleep quality. They often occur during REM sleep. The awakening itself fragments your sleep, reducing total sleep time and efficiency. The lingering anxiety or stress from the nightmare can then make it difficult to fall back asleep (sleep maintenance insomnia), further impacting restfulness. While occasional nightmares are normal, frequent ones might be linked to stress, anxiety, trauma, or certain medications, and are worth discussing with a healthcare professional if they significantly disrupt your sleep or well-being.\"},\n",
        "    {\"instruction\": \"Explain the role of Ghrelin and Leptin in relation to sleep.\", \"response\": \"Ghrelin and Leptin are key hormones regulating hunger and fullness, and they are significantly influenced by sleep. **Ghrelin** is the 'hunger hormone' – it stimulates appetite. **Leptin** is the 'satiety hormone' – it signals fullness and suppresses appetite. When you are sleep-deprived, your body tends to produce **more ghrelin** and **less leptin**. This hormonal imbalance leads to increased feelings of hunger, reduced feelings of fullness, and often cravings for high-calorie, high-carbohydrate foods, contributing to the link between poor sleep and weight gain.\"},\n",
        "    {\"instruction\": \"What is Orexin (Hypocretin) and how does it relate to wakefulness?\", \"response\": \"Orexin (also called Hypocretin) is a crucial neurotransmitter produced in the hypothalamus. Its primary role is to **promote wakefulness and alertness**. Orexin neurons activate other wakefulness-promoting centers in the brain. The loss of these orexin-producing neurons is the underlying cause of narcolepsy, a condition characterized by excessive daytime sleepiness and sudden transitions into sleep. Maintaining stable orexin levels during the day is essential for sustained alertness.\"},\n",
        "    {\"instruction\": \"Beyond light, what other 'zeitgebers' (time-givers) influence our circadian rhythm?\", \"response\": \"While light (especially sunlight) is the most powerful 'zeitgeber' or external cue that synchronizes our internal circadian clock, other factors play a role too. These include: 1. **Meal Timing:** Regularly timed meals can help anchor the body's internal rhythms. 2. **Activity/Exercise Timing:** Regular physical activity patterns contribute to the cycle. 3. **Social Interaction:** Scheduled social engagements can act as cues. 4. **Temperature:** Environmental temperature cycles influence body temperature rhythms. Consistent timing of these cues reinforces a stable circadian rhythm, complementing the effect of the light-dark cycle.\"},\n",
        "    {\"instruction\": \"I work night shifts sometimes. How can I minimize the disruption to my sleep schedule?\", \"response\": \"Working night shifts is inherently disruptive to the natural circadian rhythm. To minimize harm: 1. **Maximize Darkness during Sleep:** Make your bedroom extremely dark (blackout curtains, eye mask) for daytime sleep. 2. **Control Light Exposure:** Wear dark sunglasses on the way home from a night shift to avoid morning light exposure. Use bright light therapy upon waking before your night shift starts. 3. **Maintain Consistency:** On workdays, stick to your daytime sleep schedule as consistently as possible. On days off, try to maintain a schedule closer to your night shift pattern or shift gradually, avoiding abrupt flip-flopping if possible. 4. **Optimize Environment:** Keep the sleep environment cool and quiet. Minimize caffeine late in your 'work night'. 5. **Strategic Napping:** A nap before the night shift can help improve alertness.\"},\n",
        "    {\"instruction\": \"What happens biologically if I consistently only get 6 hours of sleep when I need 8?\", \"response\": \"Consistently getting 6 hours when you need 8 results in chronic partial sleep deprivation, leading to cumulative 'sleep debt'. Biologically, this has several consequences: cognitive functions decline (attention, memory, decision-making), emotional regulation suffers (increased irritability, stress sensitivity), immune function weakens (making you more susceptible to illness), metabolic health is impacted (increased risk of weight gain, insulin resistance), stress hormone levels (cortisol) can become dysregulated, and your perception of performance often decreases less than your actual performance impairment (you don't realize how impaired you are).\"},\n",
        "    {\"instruction\": \"Combined Data: Diary: Ate pizza and drank 2 glasses of wine at 9 PM. Bed 11 PM, tossed/turned until 12:30 AM. Woke 3 times feeling hot/uncomfortable. Alarm 7 AM, feel sluggish. Watch Data: TST 5h 45m, Eff 72%, Deep 6%, REM 14%, RHR 68, HRV 35. Analysis?\", \"response\": \"The combination of your diary and watch data clearly points to poor sleep quality driven by pre-bed choices. Eating a heavy meal (pizza) and drinking alcohol late significantly disrupted your sleep. This likely caused indigestion and elevated body temperature (feeling hot), contributing to the difficulty falling asleep (1.5h latency) and multiple awakenings. The result is very low total sleep (5h 45m), poor efficiency (72%), severely suppressed Deep (6%) and REM (14%) sleep, and physiological stress indicators (high RHR 68, very low HRV 35). Avoiding large meals and alcohol within 2-3 hours of bedtime is crucial for better sleep.\"},\n",
        "    {\"instruction\": \"My usual RHR is 55, HRV 60. Last night after a very stressful presentation: RHR 58, HRV 55. I slept 7.5 hours. Why aren't the metrics worse?\", \"response\": \"It's interesting that your sleep metrics weren't more significantly impacted after a very stressful day. While RHR is slightly up and HRV slightly down, the changes aren't dramatic. This could mean a few things: 1. Your stress response might not have lingered strongly into the night. 2. You might have good physiological resilience to stress. 3. The 7.5 hours of sleep, even if slightly less restorative than your absolute best, was sufficient to prevent a major deviation in the metrics measured by your device. It shows that sometimes the *perception* of stress doesn't perfectly correlate one-to-one with overnight physiological metrics, though often it does.\"},\n",
        "    {\"instruction\": \"If sleep efficiency is king, should I just restrict my time in bed to exactly match my average sleep time? Say, 6.5 hours in bed if I average 6.5 hours sleep?\", \"response\": \"That technique, called Sleep Restriction Therapy, is a core component of Cognitive Behavioral Therapy for Insomnia (CBT-I), but it should ideally be done under professional guidance. The goal *is* to increase sleep efficiency by reducing time spent awake in bed. However, initially restricting time in bed so drastically can lead to significant daytime sleepiness. It's usually implemented by slightly reducing time in bed (e.g., ensuring TIB isn't much longer than current TST + 30 mins) and then *gradually* increasing TIB as efficiency improves above 85-90%, without sacrificing efficiency. It's a specific therapeutic technique, not just a simple self-help trick.\"},\n",
        "    {\"instruction\": \"Can specific foods promote sleep? Like turkey or warm milk?\", \"response\": \"The idea that specific foods strongly promote sleep is mostly based on weak evidence or myth. Turkey contains tryptophan, an amino acid precursor to serotonin and melatonin, but likely not in high enough amounts relative to other amino acids to have a significant sedative effect. Warm milk's effect is often attributed more to the comforting ritual and warmth than a specific chemical property. While a balanced diet supports overall health, including sleep, relying on specific 'magic' foods isn't a primary strategy. Avoiding caffeine, alcohol, and heavy meals close to bed is generally more impactful.\"},\n",
        "    {\"instruction\": \"Explain sleep cycles and how they change through the night.\", \"response\": \"We cycle through different stages of sleep roughly every 90-110 minutes. A typical cycle starts with light NREM sleep (N1, then N2), progresses into deep NREM sleep (N3), and then transitions into REM sleep. After the REM stage, the cycle often repeats, possibly with a brief awakening. Importantly, the *proportion* of stages within each cycle changes as the night progresses. The cycles early in the night (first third) are typically dominated by **Deep Sleep (N3)**. As the night goes on, Deep Sleep decreases, and the duration of **REM sleep** periods increases, becoming longest in the cycles closer to morning.\"},\n",
        "    {\"instruction\": \"Is it true that 'social jetlag' from different weekend schedules is bad?\", \"response\": \"Yes, 'social jetlag' refers to the mismatch between your sleep schedule on work/school days versus free days (like weekends). Staying up much later and sleeping in significantly on weekends creates a discrepancy similar to traveling across time zones. This disrupts your body's internal clock (circadian rhythm), which can lead to difficulty falling asleep on Sunday night, trouble waking up on Monday morning, daytime sleepiness, reduced performance, and potentially contribute to metabolic health issues long-term. Maintaining a relatively consistent sleep schedule (+/- 1 hour) even on weekends is generally recommended to minimize social jetlag.\"},\n",
        "    {\"instruction\": \"Wearable says my SpO2 (oxygen saturation) dropped to 88% briefly last night. Is that okay?\", \"response\": \"Brief, occasional dips in oxygen saturation (SpO2) can sometimes occur during sleep, but dropping to 88% is generally considered quite low (normal is typically 95% or higher). While wearables aren't medical-grade devices and accuracy can vary, readings this low, even if brief, warrant attention. Frequent or significant drops could indicate potential breathing issues during sleep. For medical concerns or diagnoses, especially regarding oxygen levels, it's essential to consult with a qualified healthcare professional. They might recommend further testing like a formal sleep study (polysomnography).\"},\n",
        "    {\"instruction\": \"How does Growth Hormone relate to sleep?\", \"response\": \"Human Growth Hormone (HGH) plays a vital role in growth, cell reproduction, and regeneration. A significant portion of the daily release of HGH occurs during deep sleep (Stage N3 or slow-wave sleep), particularly earlier in the night. This highlights the importance of sufficient deep sleep for physical repair, muscle growth, bone health, and overall recovery. Consistently getting inadequate deep sleep can potentially impair these restorative processes linked to HGH release.\"},\n",
        "    {\"instruction\": \"I took melatonin 1mg last night and still couldn't sleep. Should I take more?\", \"response\": \"Melatonin dosage and effectiveness can be complex. While 1mg is a relatively low dose, simply increasing the dose doesn't always lead to better results for general insomnia and can sometimes increase side effects like grogginess. Melatonin works best for shifting sleep timing (like jet lag) rather than acting as a strong sedative. If 1mg didn't help, it might be that melatonin isn't the right approach for your specific sleep difficulty, or the timing was off. It's best to discuss persistent sleep issues and supplement strategies with a healthcare professional rather than self-adjusting dosages significantly.\"},\n",
        "    {\"instruction\": \"Is morning sunlight really that important for setting my body clock?\", \"response\": \"Yes, exposure to bright light, particularly natural sunlight, shortly after waking up in the morning is arguably the most powerful signal for synchronizing your body's internal clock (circadian rhythm). This morning light exposure helps suppress melatonin production, reinforces the start of your 'daytime' phase, boosts alertness, and helps ensure you'll feel appropriately sleepy later that evening. Even 10-20 minutes of outdoor morning light can make a significant difference in regulating your sleep-wake cycle.\"},\n",
        "    {\"instruction\": \"Can screen 'Night Shift' or 'Blue Light Filter' modes really help sleep?\", \"response\": \"Features like 'Night Shift' or blue light filters work by reducing the amount of blue-wavelength light emitted by screens, making the light warmer (more yellow/orange). Since blue light is the most potent suppressor of melatonin, reducing exposure in the evening *can* be helpful and is certainly better than using a screen at full brightness/color temperature. However, these filters don't eliminate all alerting effects of light, nor do they address the mental stimulation from screen content itself. They are a helpful tool, but best used as *part* of a broader strategy that includes dimming overall lights and stopping screen use altogether 30-60 minutes before bed.\"},\n",
        "    {\"instruction\": \"Combined Data: Watch: TST 6h 30m, Eff 80%, Deep 10%, REM 16%. Diary: Woke up 3 times to use the bathroom, felt thirsty. Drank lots of water right before bed. Connection?\", \"response\": \"There's a very strong connection here. Drinking large amounts of fluids right before bed often leads to needing to urinate during the night, causing awakenings like the three times you noted. These awakenings severely fragment sleep, explaining the low efficiency (80%) and likely contributing to the low Deep (10%) and REM (16%) percentages, resulting in shorter total sleep time (6h 30m). While staying hydrated is important, try to drink most of your fluids earlier in the day and reduce intake significantly in the 1-2 hours before bedtime to minimize nighttime bathroom trips.\"},\n",
        "    {\"instruction\": \"My HRV is consistently high (avg 90ms), but my sleep duration is often only 6 hours due to busy schedule. Is high HRV enough?\", \"response\": \"Having a consistently high HRV (like 90ms) is generally a great sign of good cardiovascular health and nervous system balance (strong parasympathetic tone). However, it doesn't fully compensate for chronic lack of sleep duration. While your *recovery capacity* might be excellent (reflected in the HRV), consistently getting only 6 hours when you likely need more still leads to accumulating sleep debt. This impacts cognitive function, immune health, and long-term well-being, even if your HRV looks good. High HRV is positive, but sufficient sleep *duration* is also essential and non-negotiable for optimal health.\"},\n",
        "    {\"instruction\": \"What's the difference between Cognitive Behavioral Therapy for Insomnia (CBT-I) and just practicing good sleep hygiene?\", \"response\": \"Sleep hygiene provides general guidelines for healthy sleep habits (consistent schedule, cool dark room, avoiding stimulants etc.). CBT-I is a structured, evidence-based *therapy* program delivered by a trained professional. It includes sleep hygiene education but goes much further, incorporating cognitive techniques (addressing unhelpful thoughts/beliefs about sleep), behavioral strategies (like stimulus control and sleep restriction tailored to the individual), and relaxation training. CBT-I is considered the first-line treatment for chronic insomnia because it addresses the underlying psychological and behavioral factors maintaining the sleep problem, whereas hygiene alone is often insufficient for established insomnia.\"},\n",
        "    {\"instruction\": \"I often feel a 'second wind' of energy around 10 PM, even if I was tired earlier. Why?\", \"response\": \"That 'second wind' phenomenon is often related to your circadian rhythm and cortisol levels. Your body's internal clock naturally promotes alertness at certain times. There's often a dip in alertness in the evening as melatonin starts to rise, but sometimes, especially if you push through that initial sleepiness (perhaps due to evening activities or light exposure), your alerting system (potentially involving a small cortisol bump or other wakefulness signals) can kick back in for a period before the sleep drive becomes overwhelming again later. It can sometimes indicate you missed your optimal 'sleep window'.\"},\n",
        "    {\"instruction\": \"Can magnesium supplements help with sleep?\", \"response\": \"Magnesium plays a role in regulating neurotransmitters involved in sleep and relaxation, like GABA. Some research suggests magnesium supplementation *might* help improve sleep quality, particularly in individuals who are deficient or have specific conditions like Restless Legs Syndrome. However, the evidence for its effectiveness for general insomnia in people with adequate magnesium levels is mixed and not as strong as for other interventions. As with any supplement, it's best to discuss it with a healthcare professional first, as excessive intake can have side effects, and they can assess if it's appropriate for you.\"},\n",
        "    {\"instruction\": \"My Whoop showed a skin temp deviation of +1.2°F last night. I also feel a bit achy. Should I take an ibuprofen and go for my run?\", \"response\": \"A skin temperature deviation of +1.2°F along with feeling achy can sometimes be an early indicator that your body is fighting something off, like an illness. While I can't give medical advice or suggest medications like ibuprofen, it's generally wise to listen to your body. Pushing through with a run when you're potentially getting sick might hinder your recovery. It might be more beneficial to prioritize rest today and see how you feel. If you continue to feel unwell or have concerns about your symptoms, consulting a healthcare professional is the best course of action.\"},\n",
        "    {\"instruction\": \"Fitbit: TST 5h 30m, Efficiency 78%. Deep 8%, REM 12%. RHR 70bpm. Diary: Extremely stressful day, argued with boss, ate takeout late. Can you analyze my stress impact?\", \"response\": \"It sounds like a very tough day, and your sleep metrics clearly reflect significant physiological stress. The short total sleep (5h 30m), low efficiency (78%), severely suppressed Deep (8%) and REM (12%) sleep, and elevated RHR (70bpm) are all classic signs of how acute stress, compounded by late eating, can disrupt sleep architecture and recovery. The argument and overall stress likely kept your sympathetic nervous system highly activated, making it difficult to wind down and achieve restorative sleep. Managing daytime stress and avoiding heavy meals close to bedtime are key areas to focus on for improvement.\"},\n",
        "    {\"instruction\": \"Apple Watch: Average SpO2 96%, lowest 91%. Respiratory Rate avg 15. Is a brief dip to 91% SpO2 during sleep a major problem?\", \"response\": \"An average SpO2 of 96% during sleep is good. Brief, isolated dips in oxygen saturation can occur for various reasons, and a single dip to 91% isn't necessarily a major problem if it was short-lived and your average remains high. However, if such dips are frequent, prolonged, or go lower, or if you have symptoms like loud snoring, gasping, or excessive daytime sleepiness, it would be important to discuss this with a healthcare professional. They can help determine if further investigation is needed, as I cannot provide medical diagnoses.\"},\n",
        "    {\"instruction\": \"Garmin Body Battery recharged from 10 to only 40 despite 6h TST. Yesterday was a very light activity day. Why so little recharge if I wasn't physically tired?\", \"response\": \"Even on a light activity day, Body Battery recharge can be poor if your sleep quality was compromised or if mental/emotional stress was high. Six hours of Total Sleep Time is also on the shorter side for full recovery for most adults. Factors that could lead to poor recharge despite low physical exertion include: high mental stress (even if you didn't 'feel' physically active, your brain was working hard), poor sleep architecture (not enough deep or REM sleep within those 6 hours), consuming alcohol, or even the early stages of an illness. Check your detailed sleep metrics (HRV, RHR during sleep, stress graph) in the Garmin app for more clues.\"},\n",
        "    {\"instruction\": \"Oura Ring Readiness 55. HRV Balance 'Pay Attention', RHR 'Pay Attention'. Sleep Score 68 ('Fair'). What's the interplay here?\", \"response\": \"This combination suggests that while your sleep duration or some aspects might have been okay (leading to a 'Fair' Sleep Score of 68), key physiological recovery markers are off, significantly impacting your overall Readiness (55). 'Pay Attention' for HRV Balance likely means your HRV was lower than your baseline, and for RHR, it was likely higher than baseline. These are strong indicators that your body didn't recover optimally overnight, possibly due to stress, illness, poor sleep quality (despite fair duration), or recent strain. The low Readiness score is a direct consequence of these physiological stress signals.\"},\n",
        "    {\"instruction\": \"Diary: Bed 12am, lights out 12:15am. Took 2mg melatonin. Woke 8am feeling groggy. Watch: TST 7h, Eff 88%, Deep 15%, REM 20%. Why the grogginess if melatonin is supposed to help?\", \"response\": \"While your sleep duration (7h) and efficiency (88%) look reasonable, and your Deep/REM percentages are okay, morning grogginess after taking melatonin can sometimes occur. This 'melatonin hangover' can happen if the dose was a bit too high for you, if it was taken too late, or if your body is sensitive to it. Melatonin primarily helps regulate the timing of sleep; it's not a strong sedative for everyone. The grogginess might also be simple sleep inertia. If you regularly experience this with melatonin, you might consider if the timing is optimal or if a lower dose (or none) might be better, but it's always good to discuss supplement use with a healthcare professional.\"},\n",
        "    {\"instruction\": \"I had a dream I was flying, then woke up. Is that REM sleep?\", \"response\": \"It's highly likely! Vivid, narrative, and often bizarre dreams like flying are most characteristic of REM (Rapid Eye Movement) sleep. Waking up directly from such a dream often means you were in an REM stage. REM sleep is when brain activity is quite high, similar to wakefulness in some ways, which contributes to these intense dream experiences.\"},\n",
        "    {\"instruction\": \"Explain the concept of 'sleep debt' and how long it takes to 'repay' it.\", \"response\": \"Sleep debt, or sleep deficit, is the cumulative effect of not getting enough sleep over days, weeks, or longer. If you need 8 hours but consistently get 6, you accumulate 2 hours of sleep debt each night. This debt impairs cognitive function, mood, and physical health. While you can 'repay' some acute sleep debt by sleeping longer for a few nights (e.g., on a weekend), it's thought that fully recovering from chronic, long-term sleep debt can take much longer, potentially days or even weeks of consistently adequate sleep. Moreover, some research suggests that some cognitive impacts of severe chronic sleep loss may not be fully reversible quickly. Preventing sleep debt by prioritizing consistent, sufficient sleep is the best strategy.\"},\n",
        "    {\"instruction\": \"What are 'hypnagogic jerks' and are they normal?\", \"response\": \"Hypnagogic jerks (or hypnic jerks) are those sudden, involuntary muscle twitches or jolts you might experience just as you're falling asleep (in the N1 stage). They can sometimes be accompanied by a sensation of falling or a brief visual flash. Yes, they are very common and generally considered a normal physiological phenomenon. Their exact cause isn't fully understood but might be related to the nervous system transitioning into sleep mode. They are usually harmless, though they can be startling.\"},\n",
        "    {\"instruction\": \"My child grinds their teeth loudly in their sleep. Is this something I should worry about?\", \"response\": \"Teeth grinding during sleep, also known as bruxism, is quite common in children and often resolves on its own. However, if it's very frequent, loud, or if your child complains of jaw pain, headaches, or you notice tooth wear, it's a good idea to mention it to their dentist or pediatrician. They can assess the situation and determine if any intervention, like a night guard, is needed. As a sleep coach, I can provide general sleep information, but specific concerns about medical or dental conditions like bruxism should be addressed by a healthcare professional.\"},\n",
        "    {\"instruction\": \"I want to know my exact chronotype. Can you tell me?\", \"response\": \"While I can discuss general concepts of chronotypes like 'morning larks' or 'night owls,' determining your exact, individual chronotype usually involves specific questionnaires (like the Morningness-Eveningness Questionnaire - MEQ) or sometimes even physiological measurements under controlled conditions. As an AI sleep coach, I don't have the tools to administer those tests or provide a formal chronotype assessment. You can find such questionnaires online or discuss it with a sleep specialist if you're looking for a detailed evaluation.\"},\n",
        "    {\"instruction\": \"What's the best way to calculate my total daily sleep need?\", \"response\": \"Determining your precise individual sleep need can be a bit tricky as it varies. A good practical method is to, when you have a period with a flexible schedule (like a vacation), go to bed when you feel tired and wake up naturally without an alarm for several consecutive days. The average amount of sleep you get during that period, when you feel well-rested, is likely close to your individual need. For most adults, this falls within the 7-9 hour range, but some individuals might naturally need slightly more or less. Pay attention to how you feel during the day on different amounts of sleep.\"},\n",
        "    {\"instruction\": \"I'm trying to improve my sleep. Should I focus on getting more Deep sleep or more REM sleep?\", \"response\": \"Both Deep sleep and REM sleep are crucial for different aspects of restoration, and you ideally want healthy amounts of both. Deep sleep is primarily for physical repair and feeling physically refreshed, while REM sleep is more for cognitive functions, memory consolidation, and emotional regulation. Rather than trying to target one specifically (which is hard to do directly), focus on overall good sleep hygiene and getting sufficient total sleep time. When you create the right conditions for healthy sleep overall, your body typically self-regulates the proportions of Deep and REM sleep according to its needs. If your tracker consistently shows very low levels of one or both despite good overall habits, then it might be worth investigating further.\"},\n",
        "    {\"instruction\": \"If I feel sleepy while driving, is it okay to just open the window or turn up the music?\", \"response\": \"Opening the window or turning up the music are temporary measures that might provide a very brief increase in alertness, but they **do not solve the underlying problem of driver fatigue** and can give a false sense of security. If you are feeling sleepy while driving, the safest and only truly effective solutions are to pull over in a safe place and take a short nap (15-20 minutes) or stop driving altogether until you are properly rested. Drowsy driving is extremely dangerous and similar in impairment to drunk driving.\"},\n",
        "    {\"instruction\": \"I keep hitting the snooze button 5 times every morning. Is this habit bad for me?\", \"response\": \"Yes, consistently hitting the snooze button can be detrimental. That fragmented, light sleep you get between alarms isn't very restorative and can actually contribute to 'sleep inertia,' making you feel groggier for longer when you finally get out of bed. It also disrupts your body's natural waking process. It's generally better to set your alarm for the latest possible time you actually need to wake up and then get out of bed when it first goes off. If you struggle with this, it might indicate you're not getting enough total sleep, or your sleep schedule needs adjustment.\"},\n",
        "    {\"instruction\": \"Tell me about the Glymphatic system and its role during sleep.\", \"response\": \"The glymphatic system is a fascinating 'waste clearance' system in the brain that is significantly more active during sleep, particularly deep sleep. Think of it like the brain's internal plumbing that flushes out metabolic byproducts and toxins that accumulate during wakefulness, including proteins like amyloid-beta, which is associated with Alzheimer's disease. During sleep, brain cells appear to shrink slightly, increasing the space between them, allowing cerebrospinal fluid to flow more effectively and clear out these waste products. This highlights another critical reason why sufficient, high-quality sleep is essential for brain health and cognitive function.\"},\n",
        "    {\"instruction\": \"I'm a new parent and my sleep is totally wrecked by my baby waking up. Any survival tips?\", \"response\": \"Being a new parent is incredibly demanding on sleep, and it's a phase where 'perfect' sleep hygiene often goes out the window. Focus on survival and maximizing rest when you can: 1. **Sleep when the baby sleeps:** This is cliché but true. Even short naps can help. 2. **Share night duties:** If you have a partner, take turns with night wakings if possible. 3. **Simplify everything else:** Lower your expectations for housework, social commitments, etc., to conserve energy. 4. **Accept help:** If friends or family offer to watch the baby so you can nap, take them up on it. 5. **Prioritize your own basic needs:** Stay hydrated and try to eat nutritious food. It's about getting through this period. Things will eventually get better as the baby's sleep matures.\"},\n",
        "    {\"instruction\": \"What's the deal with 'polyphasic sleep' schedules like Uberman? Are they effective?\", \"response\": \"Polyphasic sleep schedules, like the Uberman schedule (which involves multiple short naps throughout the day with very little or no core night sleep), are highly controversial and generally not recommended by sleep science. While some individuals claim to adapt, the vast majority of people cannot sustain these schedules long-term without significant performance decrements, cognitive impairment, and accumulating severe sleep debt. Our biology is strongly wired for a monophasic or biphasic (one long night sleep + optional short nap) pattern, anchored by a consolidated nighttime sleep period. Extreme polyphasic schedules often lead to chronic sleep deprivation.\"},\n",
        "    {\"instruction\": \"Combined: Diary: Bed 10:30pm, asleep by 11. Woke 7:30am. Feeling good. Watch: TST 8h, Eff 94%, Deep 2h (25%), REM 2h (25%). Last night RHR 50, HRV 70. Tonight: RHR 58, HRV 50. Had two glasses of wine with dinner tonight around 7pm. Could that explain the HRV/RHR change despite good sleep duration?\", \"response\": \"Yes, that's a very likely explanation. Even though your sleep duration and perceived quality were good both nights, and your sleep stages were excellent, alcohol consumption can significantly impact overnight recovery metrics. The two glasses of wine, even a few hours before bed, likely caused your sleeping Heart Rate (RHR) to be higher (58 vs 50) and your Heart Rate Variability (HRV) to be lower (50 vs 70) compared to the night without alcohol. Alcohol is a stressor on the body, and these physiological metrics reflect that, even if you still managed to get a good amount of sleep. This highlights how Whoop/Garmin etc. can pick up on subtle physiological stress.\"},\n",
        "    {\"instruction\": \"User asking about 'how to lucid dream'.\", \"response\": \"Lucid dreaming, where you become aware that you're dreaming while still in the dream, is a fascinating topic. Techniques often discussed include reality testing during the day (regularly asking yourself if you're dreaming), keeping a dream journal to improve dream recall, and specific methods like Mnemonic Induction of Lucid Dreams (MILD) or Wake-Back-to-Bed (WBTB). There's a lot of information available online from lucid dreaming communities and researchers if you'd like to explore those techniques further. As a sleep coach, I can point you to the concept, but I don't provide specific training protocols for it.\"},\n",
        "    {\"instruction\": \"I want to optimize my bedroom for sleep. Give me a quick checklist.\", \"response\": \"Great idea! For an optimal sleep bedroom: 1. **Darkness:** As dark as possible (blackout curtains, cover LEDs). 2. **Quiet:** Minimize noise (earplugs, white noise machine if needed). 3. **Cool Temperature:** Aim for 60-67°F (15-19°C). 4. **Comfortable Bed:** Supportive mattress, comfortable pillows and bedding. 5. **No Clutter:** A tidy room can be more calming. 6. **No Screens:** Remove TVs, computers if possible, and avoid using phones in bed. 7. **Good Air Quality:** Ensure decent ventilation.\"},\n",
        "    {\"instruction\": \"My watch sometimes says I have '0 minutes Deep Sleep'. Is that possible or a sensor error?\", \"response\": \"While it's *highly unlikely* to have absolutely zero minutes of Deep Sleep if you slept for any significant duration, consumer wearables can sometimes misclassify sleep stages, especially if sleep is very fragmented, movement artifacts are high, or sensor contact is poor. If it's an isolated incident and other metrics look okay and you feel rested, it might be a sensor or algorithm quirk for that night. However, if your watch *consistently* reports very little or no Deep Sleep, and you also feel unrefreshed, it would be worth looking into improving sleep hygiene. If concerns persist, discussing it with a healthcare professional who might suggest a clinical sleep study for accurate staging could be considered, as I cannot diagnose sensor issues or medical conditions.\"},\n",
        "    {\"instruction\": \"I'm worried I have Restless Legs Syndrome. My legs feel creepy-crawly at night when I try to relax in bed.\", \"response\": \"The symptoms you're describing—an uncomfortable, creepy-crawly sensation in the legs at rest, especially in the evening or at night, with an urge to move them—are characteristic of Restless Legs Syndrome (RLS). RLS is a neurological condition that can significantly disrupt sleep. For a proper diagnosis and to discuss potential management strategies, it's very important to consult with a healthcare professional. They can evaluate your symptoms thoroughly.\"},\n",
        "    {\"instruction\": \"What is the best sleeping position for health?\", \"response\": \"There isn't one single 'best' sleeping position that suits everyone, as it can depend on individual health conditions. However, generally, sleeping on your **back** (supine) is often considered good for spinal alignment and can help with acid reflux if your head is slightly elevated. Sleeping on your **side** (especially the left side) is also often recommended, particularly for pregnant women, people with acid reflux, or those who snore or have mild sleep apnea (as it helps keep airways open). Sleeping on your **stomach** (prone) is generally least recommended as it can strain the neck and spine. The most important thing is to find a position that is comfortable for you and allows for restful sleep.\"},\n",
        "    {\"instruction\": \"Can you give me some specific examples of a 'relaxing wind-down routine'?\", \"response\": \"Certainly! A relaxing wind-down routine helps signal to your body and mind that it's time to prepare for sleep. Aim for 30-60 minutes of calm activities. Examples include: 1. Taking a warm (not hot) bath or shower. 2. Reading a physical book under dim, warm light (avoid exciting thrillers!). 3. Listening to calm, relaxing music or a soothing podcast/audiobook. 4. Gentle stretching or yoga. 5. Practicing mindfulness meditation or deep breathing exercises. 6. Journaling – writing down worries or thoughts from the day to get them out of your head. The key is to choose activities you find personally calming and to do them consistently each night.\"}\n",
        "]\n",
        "\n",
        "# --- Save Data to JSON Lines file ---\n",
        "dataset_filename = \"sleep_coach_finetuning_data.jsonl\" # Keep standard filename\n",
        "print(f\"\\n💾 Saving {len(fine_tuning_examples)} examples to '{dataset_filename}'...\")\n",
        "try:\n",
        "    # Reasoning: Open the file in write mode ('w'). Iterate through the list of dictionaries.\n",
        "    # Use json.dump to convert each dictionary to a JSON string and write it to the file.\n",
        "    # Write a newline character ('\\n') after each JSON object to create the JSON Lines format.\n",
        "    with open(dataset_filename, 'w') as f:\n",
        "        for entry in fine_tuning_examples:\n",
        "            json.dump(entry, f)\n",
        "            f.write('\\n')\n",
        "    print(\"✅ Data saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error saving data to {dataset_filename}: {e}\")\n",
        "\n",
        "# --- Load Data using Hugging Face Datasets ---\n",
        "sleep_dataset = None # Initialize variable\n",
        "# Reasoning: Define the expected structure (Features) of the dataset for clarity and basic validation during loading.\n",
        "expected_features = Features({'instruction': Value('string'), 'response': Value('string')})\n",
        "if os.path.exists(dataset_filename):\n",
        "    print(f\"\\n⏳ Loading dataset from '{dataset_filename}'...\")\n",
        "    try:\n",
        "        # Reasoning: Load the dataset from the saved JSON Lines file using datasets.load_dataset.\n",
        "        # Specify 'json' as the format type. 'split=train' indicates this data is for training.\n",
        "        sleep_dataset = load_dataset('json', data_files=dataset_filename, features=expected_features, split='train')\n",
        "        print(\"✅ Dataset loaded successfully:\")\n",
        "        print(f\"   Number of examples: {len(sleep_dataset)}\")\n",
        "        print(\"\\n   First example in loaded dataset:\")\n",
        "        # Reasoning: Display the first example to visually confirm data integrity after loading.\n",
        "        print(sleep_dataset[0])\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading dataset from file: {e}\")\n",
        "        print(\"   Ensure the file was saved correctly and is valid JSON Lines format.\")\n",
        "else:\n",
        "    print(f\"❌ Error: Dataset file '{dataset_filename}' not found. Cannot load.\")\n",
        "\n",
        "# @title Step 9: Configure and Run Fine-Tuning (PEFT/LoRA)\n",
        "# Reasoning: This cell adapts the pre-trained LLM to the specific task of sleep coaching\n",
        "# using the custom dataset prepared in the previous step. It uses Parameter-Efficient Fine-Tuning (PEFT)\n",
        "# with LoRA (Low-Rank Adaptation) to make this feasible on Colab's limited resources.\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import time\n",
        "import datetime\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- Prerequisites Check ---\n",
        "# Reasoning: Ensure the necessary components (base model, tokenizer, dataset) are loaded and ready before starting the expensive fine-tuning process.\n",
        "print(\"--- Checking Prerequisites for Fine-Tuning ---\")\n",
        "ready_to_train = True\n",
        "if 'llm_model' not in locals() or llm_model is None:\n",
        "    print(\"❌ Prerequisite Failed: Base LLM model ('llm_model') not loaded. Run Step 4.\")\n",
        "    ready_to_train = False\n",
        "if 'llm_tokenizer' not in locals() or llm_tokenizer is None:\n",
        "    print(\"❌ Prerequisite Failed: LLM Tokenizer ('llm_tokenizer') not loaded. Run Step 4.\")\n",
        "    ready_to_train = False\n",
        "if 'sleep_dataset' not in locals() or sleep_dataset is None:\n",
        "    print(\"❌ Prerequisite Failed: Fine-tuning dataset ('sleep_dataset') not loaded. Run Step 8.\")\n",
        "    ready_to_train = False\n",
        "\n",
        "if not ready_to_train:\n",
        "    print(\"\\n🛑 Cannot proceed with fine-tuning due to missing prerequisites.\")\n",
        "else:\n",
        "    print(\"✅ Prerequisites met. Configuring fine-tuning...\\n\")\n",
        "\n",
        "    # --- Fine-Tuning Configuration ---\n",
        "    # Reasoning: Set the parameters that control the fine-tuning process.\n",
        "    output_dir = \"./sleep-coach-lora-adapter-mistral7b\" # Directory where the resulting adapter weights will be saved.\n",
        "    num_train_epochs = 3      # How many times the model will see the entire training dataset. More epochs can lead to better learning but also overfitting, especially with small datasets. Start with 1-5.\n",
        "    per_device_train_batch_size = 2 # How many training examples to process on the GPU at once. Limited by GPU memory (VRAM). Small values like 1, 2, 4 are common on Colab T4.\n",
        "    gradient_accumulation_steps = 8 # Simulate a larger batch size by accumulating gradients over multiple smaller steps. Effective batch size = batch_size * grad_accum = 2 * 8 = 16. Helps stabilize training.\n",
        "    learning_rate = 2e-4      # Controls how much the model weights are adjusted during training. 1e-4 to 5e-4 is a common range for LoRA.\n",
        "    max_seq_length = 512      # Maximum number of tokens in a sequence fed to the model. Longer sequences require more memory. Truncate or pad examples to this length.\n",
        "    logging_steps = 5         # How often (in training steps) to print the training loss. Useful for monitoring progress.\n",
        "    save_steps = 25           # How often to save a checkpoint of the LoRA adapter during training. Useful for resuming or selecting the best adapter. Adjust based on dataset size.\n",
        "    lora_r = 16               # LoRA rank (dimension of the low-rank matrices). Controls the number of trainable parameters in the adapter. 8, 16, 32, 64 are common values.\n",
        "    lora_alpha = 32           # LoRA scaling factor. Often set to 2 * lora_r.\n",
        "    lora_dropout = 0.05       # Dropout rate applied to LoRA layers to prevent overfitting.\n",
        "\n",
        "    # --- Prepare Model for PEFT/Quantized Training ---\n",
        "    print(\"⏳ Preparing base model for PEFT...\")\n",
        "    # Reasoning: Enable gradient checkpointing within the base model. This saves significant memory during training\n",
        "    # by recomputing activations during the backward pass instead of storing them all.\n",
        "    if hasattr(llm_model, 'gradient_checkpointing_enable'):\n",
        "         llm_model.gradient_checkpointing_enable()\n",
        "    # Reasoning: Prepare the bitsandbytes-quantized model for training with PEFT. This might involve minor layer adjustments.\n",
        "    llm_model_prepared = prepare_model_for_kbit_training(llm_model)\n",
        "    print(\"✅ Base model prepared.\")\n",
        "\n",
        "    # --- Define LoRA Configuration ---\n",
        "    # Reasoning: Configure the LoRA adapter layers that will be added to the base model.\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        # Reasoning: Specify which layers/modules of the base model to apply LoRA to.\n",
        "        # For Mistral-7B, targeting the query ('q_proj') and value ('v_proj') projection layers in the attention mechanism is standard and effective.\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\", # Typically set to 'none' for LoRA, meaning biases in the adapted layers are not trained.\n",
        "        task_type=TaskType.CAUSAL_LM # Specify the task type (important for PEFT).\n",
        "    )\n",
        "    print(f\"✅ LoRA configured (r={lora_r}, alpha={lora_alpha}).\")\n",
        "\n",
        "    # --- Create PEFT Model ---\n",
        "    # Reasoning: Apply the LoRA configuration to the prepared base model. This doesn't duplicate the base model,\n",
        "    # but rather inserts the small, trainable LoRA adapter layers into the specified target modules.\n",
        "    peft_model = get_peft_model(llm_model_prepared, lora_config)\n",
        "    print(\"✅ PEFT model created by adding LoRA adapter.\")\n",
        "    # Reasoning: Display information about the trainable parameters. This highlights the efficiency of LoRA –\n",
        "    # typically <1% of the total parameters are trained.\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "    # --- Preprocess & Tokenize Dataset for Training ---\n",
        "    # Reasoning: Define a function to format each instruction/response pair into the single text sequence\n",
        "    # required for Causal LM fine-tuning, using the specific format for Mistral Instruct. Then, tokenize this text.\n",
        "    def preprocess_finetune_data(examples):\n",
        "        # Format: <s>[INST] Instruction [/INST] Response</s>\n",
        "        # Add EOS token after the response to signal completion.\n",
        "        texts = [f\"<s>[INST] {instr} [/INST] {resp}{llm_tokenizer.eos_token}\"\n",
        "                 for instr, resp in zip(examples['instruction'], examples['response'])]\n",
        "        # Reasoning: Tokenize the formatted text. padding=\"max_length\" ensures all sequences have the same length (max_seq_length),\n",
        "        # adding padding tokens where needed. truncation=True cuts off sequences longer than max_seq_length.\n",
        "        return llm_tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_seq_length)\n",
        "\n",
        "    print(f\"\\n⏳ Preprocessing and tokenizing dataset (max_seq_length={max_seq_length})...\")\n",
        "    # Reasoning: Apply the preprocessing function to the entire dataset using .map(). batched=True speeds this up.\n",
        "    # remove_columns=... removes the original text columns, leaving only the tokenized 'input_ids', 'attention_mask', etc.\n",
        "    tokenized_dataset = sleep_dataset.map(preprocess_finetune_data, batched=True, remove_columns=sleep_dataset.column_names)\n",
        "    print(f\"✅ Dataset ready for training ({len(tokenized_dataset)} examples).\")\n",
        "\n",
        "    # --- Configure Training Arguments ---\n",
        "    # Reasoning: Set up the detailed configuration for the Hugging Face Trainer class.\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,                     # Directory to save checkpoints and logs.\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        logging_steps=logging_steps,               # Log metrics every N steps.\n",
        "        save_steps=save_steps,                     # Save checkpoint every N steps.\n",
        "        fp16=True,                                 # Enable mixed-precision training (uses float16). Necessary for speed and memory saving on T4 with bitsandbytes.\n",
        "        # bf16=False,                              # bf16 (bfloat16) is another mixed precision type, often better on newer GPUs (Ampere+), but fp16 is generally safer on T4.\n",
        "        gradient_checkpointing=True,               # Reconfirm gradient checkpointing is enabled (saves memory).\n",
        "        optim=\"paged_adamw_8bit\",                  # Optional: Use a memory-efficient AdamW optimizer variant from bitsandbytes. Can help prevent OOM errors.\n",
        "        save_total_limit=3,                        # Keep only the latest 3 checkpoints to save disk space.\n",
        "        dataloader_num_workers=2,                  # Use multiple workers for loading data in parallel (can speed up training slightly).\n",
        "        # lr_scheduler_type=\"cosine\",              # Optional: Use a learning rate scheduler (e.g., cosine decay) for potentially better convergence.\n",
        "        # warmup_steps=int(0.03 * len(tokenized_dataset) * num_train_epochs / gradient_accumulation_steps), # Optional: Number of initial steps with a lower learning rate to stabilize training.\n",
        "        report_to=\"none\",                          # Disable reporting to external platforms like Weights & Biases or TensorBoard.\n",
        "        # evaluation_strategy=\"steps\",             # Optional: Enable evaluation during training (requires eval_dataset).\n",
        "        # eval_steps=save_steps,                   # Optional: Evaluate every N steps.\n",
        "    )\n",
        "\n",
        "    # --- Initialize Trainer ---\n",
        "    # Reasoning: The Trainer class orchestrates the entire fine-tuning loop.\n",
        "    trainer = Trainer(\n",
        "        model=peft_model,                           # Pass the PEFT model (adapter + base) to be trained.\n",
        "        args=training_args,                         # Pass the training configuration.\n",
        "        train_dataset=tokenized_dataset,            # Pass the prepared training dataset.\n",
        "        # eval_dataset=None,                        # Optional: Pass a validation dataset for evaluation.\n",
        "        tokenizer=llm_tokenizer,                    # Pass the tokenizer for data collation.\n",
        "        # Reasoning: Handles batching and padding. DataCollatorForLanguageModeling dynamically pads sequences in each batch to the longest sequence in that batch (more efficient than padding all to max_seq_length). mlm=False for Causal LM.\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=llm_tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    # --- Start Fine-Tuning ---\n",
        "    print(\"\\n--- 🚀 Initiating Fine-Tuning ---\")\n",
        "    start_time = time.time()\n",
        "    # Reasoning: This command executes the training loop based on the provided configuration.\n",
        "    # It will iterate through the data, calculate loss, update adapter weights, log progress, and save checkpoints.\n",
        "    train_result = trainer.train()\n",
        "    end_time = time.time()\n",
        "    print(\"--- ✅ Fine-Tuning Completed ---\")\n",
        "\n",
        "    # --- Save Final LoRA Adapter ---\n",
        "    # Reasoning: After training finishes, explicitly save the final state of the LoRA adapter weights and configuration.\n",
        "    final_adapter_path = os.path.join(output_dir, \"final_adapter\")\n",
        "    print(f\"\\n💾 Saving final trained LoRA adapter to: {final_adapter_path}\")\n",
        "    peft_model.save_pretrained(final_adapter_path)\n",
        "    # Reasoning: Also save the tokenizer configuration alongside the adapter. This ensures that the correct tokenizer\n",
        "    # can be easily loaded later when loading the fine-tuned model.\n",
        "    llm_tokenizer.save_pretrained(final_adapter_path)\n",
        "    print(\"✅ Final adapter and tokenizer saved.\")\n",
        "\n",
        "    # --- Log Metrics and Duration ---\n",
        "    # Reasoning: Provide summary information about the training run.\n",
        "    training_duration_seconds = end_time - start_time\n",
        "    training_duration_formatted = str(datetime.timedelta(seconds=training_duration_seconds))\n",
        "    print(f\"\\n⏱️ Total Fine-tuning Duration: {training_duration_formatted}\")\n",
        "    # Optional: Save training metrics if needed\n",
        "    # trainer.log_metrics(\"train\", train_result.metrics)\n",
        "    # trainer.save_metrics(\"train\", train_result.metrics)\n",
        "    # trainer.save_state() # Saves optimizer state etc. if you need to resume training later\n",
        "\n",
        "    # --- Clean Up GPU Memory ---\n",
        "    # Reasoning: Explicitly delete the large model and trainer objects and clear PyTorch's GPU cache\n",
        "    # to free up memory for subsequent steps, especially important in Colab's limited environment.\n",
        "    print(\"\\n🧹 Cleaning up GPU memory...\")\n",
        "    del peft_model\n",
        "    del llm_model_prepared # Delete the prepared model too\n",
        "    # Keep llm_model (original base) if you want to compare later, otherwise delete it too:\n",
        "    # del llm_model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"✅ Memory cleanup attempted.\")\n",
        "\n",
        "# @title Step 10: Load Fine-Tuned Model (Apply LoRA Adapter to Base)\n",
        "# Reasoning: Load the original base LLM again (if it was unloaded or runtime restarted)\n",
        "# and then apply the trained LoRA adapter weights onto it to create the final, fine-tuned model for inference.\n",
        "\n",
        "from peft import PeftModel # Class for loading PEFT adapters.\n",
        "import torch\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # Ensure imports are available\n",
        "\n",
        "# --- Configuration ---\n",
        "# Reasoning: Define the path to the directory where the FINAL adapter was saved by the Trainer in Step 9.\n",
        "final_adapter_path = \"./sleep-coach-lora-adapter-mistral7b/final_adapter\" # Ensure this matches the save path from Step 9.\n",
        "\n",
        "# --- Function to Reload Base Model if Necessary ---\n",
        "# Reasoning: Encapsulate the base model loading logic to avoid repetition.\n",
        "def reload_base_model_and_tokenizer(model_id, quant_config):\n",
        "    print(f\"⏳ Reloading base model '{model_id}' with quantization...\")\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        print(\"✅ Base model reloaded.\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"right\"\n",
        "        print(\"✅ Tokenizer reloaded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to reload base model/tokenizer: {e}\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# --- Load Base Model (Only if it doesn't exist) ---\n",
        "# Reasoning: Check if the 'llm_model' object still exists in memory. If not (e.g., due to cleanup or restart), reload it.\n",
        "if 'llm_model' not in locals() or llm_model is None:\n",
        "    print(\"Base model not found in memory. Attempting reload...\")\n",
        "    # Ensure necessary config exists (might be lost after restart)\n",
        "    if 'base_model_id' not in locals(): base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "    if 'bnb_config' not in locals(): bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
        "    llm_model, llm_tokenizer = reload_base_model_and_tokenizer(base_model_id, bnb_config)\n",
        "elif 'llm_tokenizer' not in locals() or llm_tokenizer is None:\n",
        "     # Handle case where only tokenizer is missing\n",
        "     print(\"Base model found, but tokenizer missing. Reloading tokenizer...\")\n",
        "     if 'base_model_id' not in locals(): base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "     llm_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "     llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
        "     llm_tokenizer.padding_side = \"right\"\n",
        "     print(\"✅ Tokenizer reloaded.\")\n",
        "\n",
        "\n",
        "# --- Load and Apply LoRA Adapter ---\n",
        "model_finetuned = None # Initialize variable for the final fine-tuned model\n",
        "# Reasoning: Check if the base model loaded successfully AND if the saved adapter directory exists.\n",
        "if llm_model is not None and llm_tokenizer is not None and os.path.exists(final_adapter_path):\n",
        "    print(f\"\\n⏳ Loading fine-tuned LoRA adapter from: '{final_adapter_path}'...\")\n",
        "    try:\n",
        "        # Reasoning: Use PeftModel.from_pretrained to load the adapter weights specified by 'final_adapter_path'\n",
        "        # and apply them dynamically ON TOP of the existing 'llm_model' object.\n",
        "        model_finetuned = PeftModel.from_pretrained(llm_model, final_adapter_path)\n",
        "\n",
        "        # --- Optional: Merge Adapter Weights ---\n",
        "        # Reasoning: You can optionally merge the LoRA weights directly into the base model's weights.\n",
        "        # This creates a standard model object without the PEFT wrapper, which might be slightly faster for inference\n",
        "        # but requires more memory initially and makes it harder to switch adapters later. Usually NOT needed just for inference.\n",
        "        # print(\"   Merging adapter weights into base model...\")\n",
        "        # model_finetuned = model_finetuned.merge_and_unload()\n",
        "        # print(\"   Weights merged.\")\n",
        "\n",
        "        # Reasoning: Set the combined model to evaluation mode. This disables layers like dropout\n",
        "        # that are only used during training, ensuring consistent inference results.\n",
        "        model_finetuned.eval()\n",
        "        print(f\"✅ Fine-tuned PEFT model ready. Using base model '{base_model_id}' + adapter '{final_adapter_path}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading LoRA adapter: {e}\")\n",
        "        print(\"   Ensure the adapter path is correct and points to the directory containing 'adapter_config.json', etc.\")\n",
        "        model_finetuned = None\n",
        "elif not os.path.exists(final_adapter_path):\n",
        "     print(f\"❌ Error: Fine-tuned adapter path not found at '{final_adapter_path}'. Did Step 9 complete and save correctly?\")\n",
        "else:\n",
        "     print(\"❌ Error: Cannot load fine-tuned adapter because the base model or tokenizer failed to load.\")\n",
        "\n",
        "import re\n",
        "\n",
        "# @title Step 11: Interact with the FINE-TUNED Sleep Coach Agent\n",
        "# Reasoning: This cell runs the main interaction loop, allowing you to chat (via text or uploaded audio)\n",
        "# with the sleep coach agent that is now using the fine-tuned LLM.\n",
        "\n",
        "import sys # For checking if running in Colab\n",
        "\n",
        "# --- Prerequisite Check ---\n",
        "print(\"--- Checking Prerequisites for Interaction ---\")\n",
        "ready_to_interact = True\n",
        "if 'model_finetuned' not in locals() or model_finetuned is None:\n",
        "    print(\"❌ Prerequisite Failed: Fine-tuned model ('model_finetuned') not loaded. Run Step 10.\")\n",
        "    ready_to_interact = False\n",
        "if 'llm_tokenizer' not in locals() or llm_tokenizer is None:\n",
        "    print(\"❌ Prerequisite Failed: LLM Tokenizer ('llm_tokenizer') not loaded. Ensure Step 4 or 10 succeeded.\")\n",
        "    ready_to_interact = False\n",
        "if 'stt_model' not in locals() or stt_model is None:\n",
        "    print(\"❌ Prerequisite Failed: STT model ('stt_model') not loaded. Run Step 2.\")\n",
        "    ready_to_interact = False\n",
        "if 'tts_voice' not in locals() or tts_voice is None:\n",
        "    print(\"❌ Prerequisite Failed: TTS voice ('tts_voice') not loaded. Run Step 3.\")\n",
        "    ready_to_interact = False\n",
        "\n",
        "if not ready_to_interact:\n",
        "    print(\"\\n🛑 Cannot start interaction loop due to missing prerequisites.\")\n",
        "else:\n",
        "    print(\"✅ Prerequisites met. Starting interaction loop...\\n\")\n",
        "\n",
        "    # --- Interaction Loop ---\n",
        "    # Reasoning: Reset the specific conversation history for the fine-tuned model before starting.\n",
        "    conversation_history_finetuned = [] # Use the dedicated history list\n",
        "    print(\"=\"*60)\n",
        "    print(\"🗣️ 💬 INTERACTING WITH FINE-TUNED NAPCOACH 💤\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Ask sleep-related questions. Try queries related to your fine-tuning data!\")\n",
        "    print(\"Commands: type 'quit' or 'exit' to end, type 'upload' for audio input.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Reasoning: Get user input from the Colab input prompt.\n",
        "            user_input_text = input(\"You: \")\n",
        "        except EOFError:\n",
        "            print(\"\\nInput stream ended. Exiting interaction.\")\n",
        "            break # Exit loop if input stream closes\n",
        "\n",
        "        # --- Handle Exit Command ---\n",
        "        if user_input_text.lower().strip() in ['quit', 'exit']:\n",
        "            farewell_message = \"Okay, ending our session. Wishing you restful sleep!\"\n",
        "            print(f\"\\nNapCoach: {farewell_message}\")\n",
        "            speak_text_and_display(farewell_message)\n",
        "            break # Exit the while loop\n",
        "\n",
        "        processed_input_for_llm = None\n",
        "        # --- Handle Audio Upload Command ---\n",
        "        if user_input_text.lower().strip() == 'upload':\n",
        "            # Reasoning: If user types 'upload', call the function defined in Step 5 to handle audio upload and transcription.\n",
        "            transcribed_text = transcribe_uploaded_audio()\n",
        "            if transcribed_text:\n",
        "                print(f\"\\n    Mapped audio to text: '{transcribed_text}'\")\n",
        "                processed_input_for_llm = transcribed_text # Use the transcribed text as input\n",
        "            else:\n",
        "                print(\"   Audio transcription failed or was cancelled. Please type your query instead.\")\n",
        "                continue # Skip to the next loop iteration without calling LLM\n",
        "\n",
        "        # --- Handle Text Input ---\n",
        "        else:\n",
        "            # Use the text directly if it wasn't the upload command\n",
        "            processed_input_for_llm = user_input_text\n",
        "\n",
        "        # --- Generate and Speak Response ---\n",
        "        # Reasoning: Only proceed if we have valid text input (either typed or from successful transcription).\n",
        "        if processed_input_for_llm and processed_input_for_llm.strip():\n",
        "            # Reasoning: Call the 'get_llm_response' function (defined in Step 6).\n",
        "            # Crucially, pass the 'model_finetuned' object and the 'conversation_history_finetuned' list.\n",
        "            assistant_response_text = get_llm_response(\n",
        "                user_message=processed_input_for_llm,\n",
        "                model_to_use=model_finetuned,        # <<< Using the FINE-TUNED model >>>model_finetuned\n",
        "                tokenizer_to_use=llm_tokenizer,\n",
        "                history=conversation_history_finetuned, # <<< Using the FINE-TUNED history >>>conversation_history_finetuned\n",
        "                is_finetuned_model=True\n",
        "            )\n",
        "\n",
        "            # Reasoning: Call the 'speak_text_and_display' function (defined in Step 7)\n",
        "            # to synthesize the generated text response into audible speech and display the player.\n",
        "            if assistant_response_text:\n",
        "              # --- Pre-process text for TTS ---\n",
        "              tts_processed_text = assistant_response_text\n",
        "\n",
        "              # 1. Handle specific, known phrases first (if any are particularly tricky for regex or common)\n",
        "              #    (You can keep some of your original replacements here if they are very specific and safe)\n",
        "              #    Example: (though the regex below should cover these specific hour ranges too)\n",
        "              #    tts_processed_text = tts_processed_text.replace(\"7-9 hours\", \"7 to 9 hours\")\n",
        "\n",
        "              # 2. Use regex to handle general numerical ranges (e.g., \"N-M\")\n",
        "              #    Pattern: (\\d+) means \"one or more digits\" (captured in group 1)\n",
        "              #             -       means a literal dash\n",
        "              #             (\\d+) means \"one or more digits\" (captured in group 2)\n",
        "              #    Replacement: r'\\1 to \\2' means \"content of group 1, followed by ' to ', followed by content of group 2\"\n",
        "              tts_processed_text = re.sub(r'(\\d+)-(\\d+)', r'\\1 to \\2', tts_processed_text)\n",
        "\n",
        "              # 3. Handle ranges with units more carefully if needed, or let the above regex handle most cases.\n",
        "              #    For instance, \"60-67 degrees Fahrenheit\" would become \"60 to 67 degrees Fahrenheit\" by the regex above.\n",
        "              #    If you have specific units that need special handling, you might add more targeted regex or string replaces.\n",
        "              #    Example for percentages (if the generic one doesn't do what you want or has side effects):\n",
        "              #    tts_processed_text = re.sub(r'(\\d+)-(\\d+)%', r'\\1 to \\2 percent', tts_processed_text)\n",
        "\n",
        "              print(f\"   TTS Input (Original): '{assistant_response_text}'\") # For debugging\n",
        "              print(f\"   TTS Input (Processed with Regex): '{tts_processed_text}'\") # For debugging\n",
        "              speak_text_and_display(tts_processed_text)\n",
        "\n",
        "        else:\n",
        "            # Handle cases where input was empty after potential upload failure etc.\n",
        "            print(\"   Please provide a valid query, type 'upload', or 'quit'.\")\n",
        "            # No LLM call needed if input is invalid/empty.\n",
        "\n",
        "    print(\"\\n--- Interaction loop finished. ---\")\n",
        "\n",
        "# @title Gradio Application Cell (RUN THIS AS THE VERY LAST CELL)\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "import shutil # For potential cleanup later, though Gradio manages its own temp for inputs\n",
        "\n",
        "# --- Ensure global variables from your notebook are accessible by this point ---\n",
        "# (Assuming all your necessary global variables and functions STT, LLM, TTS models & functions are loaded)\n",
        "# stt_model, model_finetuned, llm_tokenizer, tts_voice, tts_sample_rate,\n",
        "# conversation_history_finetuned, SYSTEM_PROMPT,\n",
        "# transcribe_audio, get_llm_response, synthesize_for_gradio_and_autoplay,\n",
        "# GRADIO_TTS_OUTPUT_DIR\n",
        "\n",
        "\n",
        "# --- Gradio Processing Function (napcoach_interface_handler) ---\n",
        "# This function remains THE SAME as in your original code.\n",
        "# No changes are needed to napcoach_interface_handler itself.\n",
        "# Make sure it's correctly defined as in your provided code.\n",
        "def napcoach_interface_handler(user_audio_filepath, user_text_query, chat_history_for_display):\n",
        "    \"\"\"\n",
        "    Handles user interaction for the NapCoach Gradio app.\n",
        "    1. Transcribes audio if provided, otherwise uses text.\n",
        "    2. Gets a response from the fine-tuned LLM, managing internal history.\n",
        "    3. Synthesizes the LLM's response to audio.\n",
        "    4. Updates the chat display.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Gradio: New Interaction ---\")\n",
        "    final_user_query_to_llm = \"\"\n",
        "    llm_response_text = \"\"\n",
        "    tts_output_audio_filepath = None\n",
        "\n",
        "    # Determine input source (audio or text)\n",
        "    if user_audio_filepath and os.path.exists(user_audio_filepath):\n",
        "        print(f\"🎤 Gradio: Processing audio input from: {user_audio_filepath}\")\n",
        "        # For progress with yield, it's better to handle it inside the generator\n",
        "        # Here, we'll assume a blocking call for simplicity as in the original.\n",
        "        # progress_bar = gr.Progress(track_tqdm=True) # If you want progress for non-yielding\n",
        "        # progress_bar.update(0, desc=\"Transcribing your voice...\")\n",
        "        final_user_query_to_llm = transcribe_audio(user_audio_filepath) # Assumes transcribe_audio is defined\n",
        "        # progress_bar.update(1.0, desc=\"Transcription complete!\")\n",
        "        if not final_user_query_to_llm:\n",
        "            final_user_query_to_llm = \"(Audio transcription failed or was empty)\"\n",
        "    elif user_text_query and user_text_query.strip():\n",
        "        print(f\"📝 Gradio: Processing text input: '{user_text_query}'\")\n",
        "        final_user_query_to_llm = user_text_query\n",
        "    else:\n",
        "        print(\"🤷 Gradio: No valid input provided by user.\")\n",
        "        yield chat_history_for_display, None, None, None, \"\" # (hist, llm_text, tts_audio, clear_audio_in, clear_text_in)\n",
        "        return\n",
        "\n",
        "    # Add user's query to Gradio's display history\n",
        "    if chat_history_for_display is None: chat_history_for_display = []\n",
        "    chat_history_for_display.append([final_user_query_to_llm, None])\n",
        "    yield chat_history_for_display, None, None, None, \"\" # Update UI with user message\n",
        "\n",
        "    # Get LLM Response\n",
        "    print(f\"🧠 Gradio: Sending to LLM: '{final_user_query_to_llm}'\")\n",
        "    if final_user_query_to_llm and final_user_query_to_llm != \"(Audio transcription failed or was empty)\":\n",
        "        # progress_bar.update(0, desc=\"NapCoach is thinking...\")\n",
        "        llm_response_text = get_llm_response( # Assumes get_llm_response is defined\n",
        "            user_message=final_user_query_to_llm,\n",
        "            model_to_use=model_finetuned,\n",
        "            tokenizer_to_use=llm_tokenizer,\n",
        "            history=conversation_history_finetuned,\n",
        "            is_finetuned_model=True\n",
        "        )\n",
        "        # progress_bar.update(1.0, desc=\"Response received!\")\n",
        "        if not llm_response_text:\n",
        "            llm_response_text = \"I'm sorry, I couldn't generate a response to that query.\"\n",
        "    else:\n",
        "        llm_response_text = \"I couldn't understand the audio. Could you please type your query?\"\n",
        "\n",
        "    print(f\"🤖 Gradio: LLM Response: '{llm_response_text}'\")\n",
        "    chat_history_for_display[-1][1] = llm_response_text # Update with bot response\n",
        "    yield chat_history_for_display, llm_response_text, None, None, \"\" # Update UI with LLM text\n",
        "\n",
        "    # Synthesize LLM's Text Response to Audio\n",
        "    if llm_response_text:\n",
        "        print(f\"🔊 Gradio: Synthesizing TTS for: '{llm_response_text}'\")\n",
        "        # progress_bar.update(0, desc=\"NapCoach is preparing audio...\")\n",
        "        tts_output_audio_filepath = synthesize_for_gradio_and_autoplay(llm_response_text) # Assumes this is defined\n",
        "        # progress_bar.update(1.0, desc=\"Audio ready!\")\n",
        "        if tts_output_audio_filepath:\n",
        "            print(f\"✅ Gradio: TTS audio ready at: {tts_output_audio_filepath}\")\n",
        "        else:\n",
        "            print(\"⚠️ Gradio: TTS synthesis failed or returned no audio path.\")\n",
        "\n",
        "    # Final yield\n",
        "    # Clear text input, audio input might clear automatically or you can return its current value to clear it\n",
        "    yield chat_history_for_display, llm_response_text, tts_output_audio_filepath, None, \"\"\n",
        "\n",
        "\n",
        "# --- Gradio Interface Definition using Blocks ---\n",
        "with gr.Blocks(theme=gr.themes.Glass(), title=\"NapCoach AI\", css=\"footer {display: none !important}\") as demo: # Added CSS to hide footer\n",
        "\n",
        "    # --- MODIFICATION 1: Simplified Header ---\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div style=\"text-align: center; padding-top: 10px; padding-bottom: 0px;\">\n",
        "            <h1 style=\"margin-bottom: 0px;\">NapCoach AI 💤</h1>\n",
        "            <p style=\"margin-top: 5px; font-size: 1.1em;\">Your personal sleep coaching assistant</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chatbot_ui = gr.Chatbot(\n",
        "        label=\"Conversation with NapCoach\",\n",
        "        bubble_full_width=False,\n",
        "        height=500,\n",
        "        # User avatar (None uses default). Bot avatar from your original code.\n",
        "        # For a user icon similar to image, find an image URL: e.g., \"https://img.icons8.com/ios-glyphs/90/663399/user-male-circle.png\"\n",
        "        avatar_images=(None, \"https://img.icons8.com/color/96/000000/bot.png\")\n",
        "    )\n",
        "\n",
        "    # This hidden textbox is a target for the LLM's text response.\n",
        "    llm_text_for_display = gr.Textbox(label=\"Coach's Text Reply (hidden)\", interactive=False, visible=False)\n",
        "\n",
        "    # --- MODIFICATION 2: Input Area Reorganized ---\n",
        "    with gr.Column(scale=1): # Main container for inputs at the bottom\n",
        "        with gr.Accordion(\"🎙️ Use Voice (Tap to expand)\", open=False):\n",
        "            audio_in_ui = gr.Audio(\n",
        "                sources=[\"microphone\", \"upload\"],\n",
        "                type=\"filepath\",\n",
        "                label=\"Record or Upload Audio Query\",\n",
        "                # format=\"wav\" # Optional\n",
        "            )\n",
        "\n",
        "        with gr.Row(variant=\"compact\", elem_id=\"input_row\"): # Compact row for text input and send\n",
        "            text_in_ui = gr.Textbox(\n",
        "                placeholder=\"Type your message here...\",\n",
        "                show_label=False,\n",
        "                lines=1,        # Single line for chat feel\n",
        "                max_lines=5,    # Allow some expansion for longer messages\n",
        "                container=False, # To make it fit better in the row potentially\n",
        "                scale=7         # Textbox takes more width\n",
        "            )\n",
        "            submit_btn_ui = gr.Button(\n",
        "                \"➤\",            # --- MODIFICATION 3: Icon-like send button ---\n",
        "                variant=\"primary\",\n",
        "                scale=1,         # Button takes less width\n",
        "                min_width=60     # Ensure button looks like a button\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"---\") # Using a thematic break (horizontal rule) for visual separation instead of div spacer\n",
        "    tts_audio_out_ui = gr.Audio(\n",
        "        label=\"🎧 NapCoach Voice Reply\", # Added emoji\n",
        "        type=\"filepath\",\n",
        "        autoplay=True,\n",
        "        interactive=False\n",
        "    )\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "    # --- MODIFICATION 4: Utility Buttons and Status (Retaining cleaner layout from previous suggestion) ---\n",
        "    def clear_internal_llm_history_action_ui_v2():\n",
        "        global conversation_history_finetuned\n",
        "        global SYSTEM_PROMPT # Make sure SYSTEM_PROMPT is accessible\n",
        "        conversation_history_finetuned = []\n",
        "        if 'SYSTEM_PROMPT' in globals() and SYSTEM_PROMPT:\n",
        "            conversation_history_finetuned.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
        "        print(\"🧹 Internal LLM conversation history cleared by UI button.\")\n",
        "        return \"NapCoach's internal memory has been reset. You can start a new conversation.\"\n",
        "\n",
        "    with gr.Row(equal_height=False): # Group utility buttons\n",
        "        clear_btn_ui = gr.ClearButton(\n",
        "            components=[audio_in_ui, text_in_ui, chatbot_ui, llm_text_for_display, tts_audio_out_ui], # Add status_text_ui here if you want this button to clear it\n",
        "            value=\"🧹 Clear Chat & Inputs\"\n",
        "        )\n",
        "        clear_llm_mem_btn_ui = gr.Button(\n",
        "            \"🧠 Reset Coach Memory\"\n",
        "        )\n",
        "\n",
        "    status_text_ui = gr.Textbox( # Status text box below buttons\n",
        "        placeholder=\"Status messages appear here.\",\n",
        "        interactive=False,\n",
        "        lines=1,\n",
        "        max_lines=1,\n",
        "        show_label=False\n",
        "    )\n",
        "    # Connect the click action for resetting memory\n",
        "    clear_llm_mem_btn_ui.click(\n",
        "        fn=clear_internal_llm_history_action_ui_v2,\n",
        "        inputs=None,\n",
        "        outputs=[status_text_ui]\n",
        "    )\n",
        "\n",
        "    # --- Event Handling (No changes here from your original logic) ---\n",
        "    gradio_inputs = [audio_in_ui, text_in_ui, chatbot_ui]\n",
        "    gradio_outputs = [chatbot_ui, llm_text_for_display, tts_audio_out_ui,\n",
        "                      audio_in_ui, text_in_ui] # Last two are for clearing inputs\n",
        "\n",
        "    # When text is submitted via Enter key in text_in_ui or submit_btn_ui is clicked\n",
        "    # We use a generator for napcoach_interface_handler\n",
        "    actions = [text_in_ui.submit, submit_btn_ui.click]\n",
        "    for action in actions:\n",
        "        action(\n",
        "            fn=napcoach_interface_handler,\n",
        "            inputs=gradio_inputs,\n",
        "            outputs=gradio_outputs,\n",
        "            show_progress=\"full\" # Show progress for generator\n",
        "        )\n",
        "\n",
        "# --- Launch the Gradio App ---\n",
        "print(\"\\n🚀 Launching Gradio Interface in Colab...\")\n",
        "demo.launch(debug=True, share=True, inbrowser=True)"
      ]
    }
  ]
}